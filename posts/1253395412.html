<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>MICRO 2025 Systems for Al (Training) 部分论文阅读 | Cosmos</title><meta name="author" content="ZJN"><meta name="copyright" content="ZJN"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="NetZlP   在分布式大模型训练中实现网络内无损压缩梯度和激活,实现了压缩算法和硬件加速器   并行训练模式总结对比表    维度 数据并行 (DP) 流水线并行 (PP) 张量并行 (TP)     并行核心思想 数据维度：将训练数据分批，在不同的设备上使用相同的模型副本进行处理。 模型维度（层间）：将模型按层拆分成多个阶段，每个设备负责模型的一个连续部分。 模型维度（层内）：将单个层内的">
<meta property="og:type" content="article">
<meta property="og:title" content="MICRO 2025 Systems for Al (Training) 部分论文阅读">
<meta property="og:url" content="http://iczjn.com/posts/1253395412.html">
<meta property="og:site_name" content="Cosmos">
<meta property="og:description" content="NetZlP   在分布式大模型训练中实现网络内无损压缩梯度和激活,实现了压缩算法和硬件加速器   并行训练模式总结对比表    维度 数据并行 (DP) 流水线并行 (PP) 张量并行 (TP)     并行核心思想 数据维度：将训练数据分批，在不同的设备上使用相同的模型副本进行处理。 模型维度（层间）：将模型按层拆分成多个阶段，每个设备负责模型的一个连续部分。 模型维度（层内）：将单个层内的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://iczjn.com/img/cover_16.jpg">
<meta property="article:published_time" content="2025-10-22T00:04:13.000Z">
<meta property="article:modified_time" content="2025-10-29T11:34:10.616Z">
<meta property="article:author" content="ZJN">
<meta property="article:tag" content="体系结构">
<meta property="article:tag" content="论文">
<meta property="article:tag" content="MICRO2025">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://iczjn.com/img/cover_16.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "MICRO 2025 Systems for Al (Training) 部分论文阅读",
  "url": "http://iczjn.com/posts/1253395412.html",
  "image": "http://iczjn.com/img/cover_16.jpg",
  "datePublished": "2025-10-22T00:04:13.000Z",
  "dateModified": "2025-10-29T11:34:10.616Z",
  "author": [
    {
      "@type": "Person",
      "name": "ZJN",
      "url": "http://iczjn.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://iczjn.com/posts/1253395412.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'MICRO 2025 Systems for Al (Training) 部分论文阅读',
  isHighlightShrink: true,
  isToc: true,
  pageType: 'post'
}</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script async="async">(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)
  setTimeout(function(){preloader.endLoading();}, 3000);

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="web_bg" style="background-image: url(/img/background.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "/img/loading.gif" data-lazy-src="/img/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">15</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/cover_16.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Cosmos</span></a><a class="nav-page-title" href="/"><span class="site-name">MICRO 2025 Systems for Al (Training) 部分论文阅读</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">MICRO 2025 Systems for Al (Training) 部分论文阅读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-22T00:04:13.000Z" title="发表于 2025-10-22 08:04:13">2025-10-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-29T11:34:10.616Z" title="更新于 2025-10-29 19:34:10">2025-10-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/IC/">IC</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="netzlp"><a class="markdownIt-Anchor" href="#netzlp"></a> NetZlP</h1>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251022081259452.png" alt="image-20251022081259452" /></p>
<blockquote>
<p>在分布式大模型训练中实现<strong>网络内无损压缩</strong>梯度和激活,实现了压缩算法和硬件加速器</p>
</blockquote>
<h3 id="并行训练模式总结对比表"><a class="markdownIt-Anchor" href="#并行训练模式总结对比表"></a> 并行训练模式总结对比表</h3>
<table>
<thead>
<tr>
<th style="text-align:left">维度</th>
<th style="text-align:left">数据并行 (DP)</th>
<th style="text-align:left">流水线并行 (PP)</th>
<th style="text-align:left">张量并行 (TP)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>并行核心思想</strong></td>
<td style="text-align:left"><strong>数据维度</strong>：将训练数据分批，在不同的设备上使用<strong>相同的模型副本</strong>进行处理。</td>
<td style="text-align:left"><strong>模型维度（层间）</strong>：将模型按<strong>层</strong>拆分成多个阶段，每个设备负责模型的一个连续部分。</td>
<td style="text-align:left"><strong>模型维度（层内）</strong>：将单个<strong>层内的运算和参数</strong>进行拆分，分布到多个设备上。</td>
</tr>
<tr>
<td style="text-align:left"><strong>如何划分</strong></td>
<td style="text-align:left">划分<strong>训练数据集</strong>。</td>
<td style="text-align:left">划分<strong>模型的层</strong>。</td>
<td style="text-align:left">划分<strong>层的权重矩阵/计算</strong>。</td>
</tr>
<tr>
<td style="text-align:left"><strong>设备上的模型状态</strong></td>
<td style="text-align:left">每个设备拥有<strong>完整模型</strong>的一个副本。</td>
<td style="text-align:left">每个设备只拥有<strong>模型的一部分</strong>（一组连续的层）。</td>
<td style="text-align:left">每个设备拥有<strong>一层或几层的部分参数</strong>。</td>
</tr>
<tr>
<td style="text-align:left"><strong>通信内容</strong></td>
<td style="text-align:left"><strong>梯度</strong>（反向传播后）</td>
<td style="text-align:left"><strong>激活值</strong>（前向传播时） <strong>梯度</strong>（反向传播时）</td>
<td style="text-align:left"><strong>部分激活值/计算结果</strong>（前向和反向传播过程中）</td>
</tr>
<tr>
<td style="text-align:left"><strong>通信时机</strong></td>
<td style="text-align:left">每次迭代的<strong>反向传播结束后</strong>。</td>
<td style="text-align:left">在前向和反向传播过程中，<strong>阶段与阶段之间</strong>。</td>
<td style="text-align:left">在<strong>单个层的前向和反向计算过程中</strong>。</td>
</tr>
<tr>
<td style="text-align:left"><strong>主要优势</strong></td>
<td style="text-align:left">- 实现简单，应用广泛。 - 对于模型较小、数据量大的情况非常有效。</td>
<td style="text-align:left">- 可以训练<strong>模型过大，无法放入单个设备显存</strong>的模型。 - 通信量相对可控。</td>
<td style="text-align:left">- 能极细粒度地降低<strong>单个巨大层</strong>的内存占用（如大模型中的FFN、Attention层）。</td>
</tr>
<tr>
<td style="text-align:left"><strong>主要挑战/代价</strong></td>
<td style="text-align:left">- 每个设备仍需容纳<strong>整个模型</strong>，对于大模型不适用。 - 全局同步梯度会产生通信瓶颈，尤其是设备数量多时。</td>
<td style="text-align:left">- 存在<strong>流水线气泡</strong>，造成设备空闲，降低计算效率。 - 需要仔细进行模型切分以平衡各阶段负载</td>
<td style="text-align:left">- <strong>通信非常频繁</strong>（每层都可能需要通信），对设备间互联带宽要求极高。》</td>
</tr>
</tbody>
</table>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1937826285264011929">(33 封私信 / 80 条消息) 一文深度全面解析大模型分布式并行策略：DP/TP/PP/CP/EP/SP - 知乎</a></p>
<p>一个讲训练模式的文章，觉得讲的不错。</p>
</blockquote>
<p>顶级超大规模集群将其每个计算节点连接到多个400 Gbps网络链路[13, 49]。然而，公共云中经济实惠的集群仍然依赖于中低带宽的网络链路。范围从低至10 Gbps到常见的50 Gbps。</p>
<p>并非所有人都能轻易负担得起访问网络带宽超过100 Gbps的高端实例。当网络带宽降低到50 Gbps时，Llama-3 70B、GPT-3 175B和Llama-3 405B的每次训练迭代的通信时间分别增加2.2×、2.0×和1.7×。</p>
<h2 id="压缩算法"><a class="markdownIt-Anchor" href="#压缩算法"></a> 压缩算法</h2>
<p>在分布式大模型训练中，梯度和激活通常采用 <strong>bfloat16</strong> 格式存储（16位浮点数）。标准无损压缩算法（如 LZ4 和 Snappy）主要依赖于<strong>字典匹配</strong>，通过滑动窗口查找重复的字符（8位字节序列）来压缩数据 。</p>
<p>bfloat16 数据的 <strong>低 8 位</strong> 接近随机分布（1的概率约为 50%），这使得标准压缩算法很难找到重复的字节序列 。实验显示，LZ4 和 Snappy 几乎无法压缩 bfloat16 格式的梯度和激活（压缩比接近 100%）。</p>
<table>
<thead>
<tr>
<th style="text-align:left">格式</th>
<th style="text-align:left">总位数</th>
<th style="text-align:left">符号位</th>
<th style="text-align:left">指数位</th>
<th style="text-align:left">尾数位</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>FP32 (float)</strong></td>
<td style="text-align:left">32 bits</td>
<td style="text-align:left">1 bit</td>
<td style="text-align:left">8 bits</td>
<td style="text-align:left">23 bits</td>
</tr>
<tr>
<td style="text-align:left"><strong>FP16 (half)</strong></td>
<td style="text-align:left">16 bits</td>
<td style="text-align:left">1 bit</td>
<td style="text-align:left">5 bits</td>
<td style="text-align:left">10 bits</td>
</tr>
<tr>
<td style="text-align:left"><strong>bfloat16</strong></td>
<td style="text-align:left">16 bits</td>
<td style="text-align:left">1 bit</td>
<td style="text-align:left">8 bits</td>
<td style="text-align:left">7 bits</td>
</tr>
</tbody>
</table>
<h3 id="位级别转换字节和比特分组"><a class="markdownIt-Anchor" href="#位级别转换字节和比特分组"></a> 位级别转换：字节和比特分组</h3>
<p>虽然尾数位是随机的，但梯度和激活的 <strong>指数位</strong> 具有较高的“1”的概率（1.07169×10−8 = 0_01100100_0111000) 。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251027090838491.png" alt="image-20251027090838491" /></p>
<p><strong>字节分组：</strong> 将所有 bfloat16 数值中的<strong>高字节</strong>（包含符号位和指数位）集中在一起，再将所有<strong>低字节</strong>（包含尾数位）集中在一起 7。</p>
<p><strong>比特分组：</strong> 将所有 bfloat16 数值中的<strong>第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 个比特</strong>（例如，所有数的第 15 位）集中在一起，依此类推 。</p>
<p>通过分组，可压缩性更高的指数位和符号位被排列到相邻位置，从而帮助 LZ4 等算法更容易识别重复模式，大幅提高字典匹配的效率 。</p>
<h3 id="值级别转换delta-压缩"><a class="markdownIt-Anchor" href="#值级别转换delta-压缩"></a> 值级别转换：Delta 压缩</h3>
<p>梯度和激活的值在训练迭代过程中是<strong>逐渐变化</strong>的 。</p>
<p>从当前迭代的每个梯度或激活值中，减去一个<strong>有代表性的基准值</strong> ：</p>
<ul>
<li>理想情况下，可以减去上一迭代的对应值（称为“真 Delta 压缩”）。</li>
<li>在 NIC 内存受限的实际场景中，论文提出为每一层使用一个<strong>单基准值</strong>，例如该层中所有值的<strong>最小值</strong> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>) 。</li>
</ul>
<p>进行减法操作后得到的**“Delta 值”<strong>会更集中于零点附近，这使得原本随机的</strong>尾数位更趋向于 0**，从而使数据整体更具可压缩性 。</p>
<p>实验证明，<strong>Delta 压缩结合比特分组</strong>辅助下的 NETZIP-LZ4，相比标准 LZ4 实现了巨大的提升 ：</p>
<p>梯度的数据量平均减少了 <strong>67%</strong> 。</p>
<p>激活的数据量平均减少了 <strong>70%</strong> 。</p>
<h2 id="压缩加速器"><a class="markdownIt-Anchor" href="#压缩加速器"></a> 压缩加速器</h2>
<p>提出了NetZIP-accelerator加速器，集成到NIC ASIC中。</p>
<p>NETZIP-accelerator 采用了**“线内”**的架构 ，意味着它将整个处理模块直接放置在数据流向网络（或从网络接收）的路径上。</p>
<p><strong>集成位置：</strong> 加速器将 NETZIP-algorithm 的转换功能与一个<strong>LZ4加速器****集成到 NIC（网卡）内部</strong> 。</p>
<p><strong>解决传输开销：</strong> 传统的压缩方式（在 CPU、GPU 或 SNIC 上运行）需要额外的内存传输 。例如，数据在发送前必须经历：发送方 GPU 内存 -&gt; CPU 内存 -&gt; CPU 压缩 -&gt; CPU 内存 -&gt; NIC 的路径 。这些<strong>额外的传输</strong>显著增加了端到端的压缩/解压缩延迟 。</p>
<p>作为“线内”加速器，NETZIP-accelerator <strong>消除了</strong>这些不必要的梯度和激活数据的<strong>额外传输</strong> ，从而直接在数据流经 NIC 时完成转换、压缩和解压缩，大幅减少了总体延迟 。</p>
<h3 id="压缩路径"><a class="markdownIt-Anchor" href="#压缩路径"></a> 压缩路径</h3>
<p>当数据从主机（GPU 内存）通过 NIC 发送到网络时，会经过压缩路径：</p>
<p><strong>组件顺序：</strong> <strong>Tx (DE)MUX</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>Delta 编码器</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>分组暂存缓冲区</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>分组器</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>压缩器</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>Tx MUX/协议引擎</strong></p>
<table>
<thead>
<tr>
<th><strong>组件</strong></th>
<th><strong>功能</strong></th>
<th><strong>机制/实现细节</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Tx (DE)MUX</strong></td>
<td><strong>流量控制/选择性压缩</strong></td>
<td>根据自定义标头中的 <strong>1 位压缩标志</strong>，决定数据包的去向： 1. 如果标志未设置（非梯度/激活数据包），直接将数据包转发给协议引擎（不压缩）。 2. 如果标志设置，将数据包导向 Delta 编码器进行压缩。</td>
</tr>
<tr>
<td><strong>Delta 编码器</strong></td>
<td><strong>值级别转换</strong></td>
<td>负责执行 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>i</mi></msub><mo>−</mo><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">G_{i} - D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 操作，由三个单元组成： 1. <strong>基值计算单元：</strong> 遍历数据包中<strong>同一层 ID</strong>（由 15 位 Layer ID 标识）的 bfloat16 值，找出该层的<strong>最小值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></strong> 作为基值。 2. <strong>基值寄存器：</strong> 存储每一层的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 基值。 3. <strong>增量值计算单元：</strong> 计算每个中间值（梯度或激活）与 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的差值，生成 <strong>Delta 值</strong>。计算完成后，将 Delta 值和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 一起传出。</td>
</tr>
<tr>
<td><strong>分组暂存缓冲区</strong></td>
<td><strong>缓存与双缓冲</strong></td>
<td>采用<strong>双缓冲架构</strong>： 1. 当一个缓冲区在<strong>接收</strong>当前 4 kB 数据包时。 2. 另一个缓冲区则存储<strong>前一个</strong>已完成 Delta 编码的数据包，供分组器使用。此设计确保了数据流的连续性，最小化延迟。</td>
</tr>
<tr>
<td><strong>分组器 (Grouper)</strong></td>
<td><strong>位级别转换</strong></td>
<td>对 Delta 值执行 <strong>字节或比特粒度</strong> 的重排列（即 NETZIP-algorithm 中的<strong>比特/字节分组</strong>），将可压缩的部分聚集在一起。</td>
</tr>
<tr>
<td><strong>压缩器 (Compressor)</strong></td>
<td><strong>无损压缩</strong></td>
<td>对分组后的 Delta 值进行 <strong>LZ4 无损压缩</strong>。压缩完成后，将<strong>基值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 附加到压缩后的有效载荷末尾</strong>（用于解压缩时的恢复）。  <strong>故障处理：</strong> 如果压缩后的大小仍超过 4 kB，则直接丢弃压缩结果，将<strong>原始未压缩数据包</strong>（移除自定义标头后）发送给协议引擎。</td>
</tr>
<tr>
<td><strong>Tx MUX</strong></td>
<td><strong>出口选择</strong></td>
<td>将压缩后的数据包（或在压缩失败时将原始数据包）发送给协议引擎，准备发往网络。</td>
</tr>
</tbody>
</table>
<h3 id="解压缩路径decompression-path"><a class="markdownIt-Anchor" href="#解压缩路径decompression-path"></a> 解压缩路径（Decompression Path）</h3>
<p>当数据从网络接收到 NIC 时，会经过解压缩路径：</p>
<p><strong>组件顺序：</strong> <strong>Rx DEMUX</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>解压缩器</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>解分组暂存缓冲区</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>解分组器</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>Delta 解码器</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>Rx MUX/DMA 引擎+缓冲区</strong></p>
<table>
<thead>
<tr>
<th><strong>组件</strong></th>
<th><strong>功能</strong></th>
<th><strong>机制/实现细节</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Rx DEMUX</strong></td>
<td><strong>流量控制/解压缩判断</strong></td>
<td>从协议引擎接收数据包后，检查其自定义标头中的 <strong>1 位压缩标志</strong>： 1. 如果未压缩，直接通过 Rx MUX 转发给 DMA 引擎+缓冲区（不解压）。 2. 如果已压缩，将数据包导向解压缩器。</td>
</tr>
<tr>
<td>**解压缩器 **</td>
<td><strong>无损解压缩</strong></td>
<td>对数据包的有效载荷进行 <strong>LZ4 解压缩</strong>。解压后的数据被写入<strong>解分组暂存缓冲区</strong>。</td>
</tr>
<tr>
<td><strong>解分组暂存缓冲区</strong></td>
<td><strong>缓存</strong></td>
<td>存储解压缩后的有效载荷。</td>
</tr>
<tr>
<td>**解分组器 **</td>
<td><strong>位级别恢复</strong></td>
<td>将解压缩后的比特或字节序列<strong>重新排列</strong>回其<strong>原始顺序</strong>（即 Delta 值的顺序）。并将解分组后的 Delta 值与<strong>附带的基值</strong>一起发送给 Delta 解码器。</td>
</tr>
<tr>
<td><strong>Delta 解码器</strong></td>
<td><strong>值级别恢复</strong></td>
<td>对解分组器传来的每个 <strong>Delta 值</strong>，执行 <strong>加法运算</strong>： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>原始值</mtext><mo>=</mo><mtext>Delta 值</mtext><mo>+</mo><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\text{原始值} = \text{Delta 值} + D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord cjk_fallback">原始值</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord text"><span class="mord">Delta </span><span class="mord cjk_fallback">值</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，从而完全恢复原始的梯度或激活值。</td>
</tr>
<tr>
<td><strong>Rx MUX</strong></td>
<td><strong>出口</strong></td>
<td>将恢复后的原始数据包发送给 DMA 引擎+缓冲区，最终传输到接收方的 GPU 内存。</td>
</tr>
</tbody>
</table>
<h3 id="asic实现"><a class="markdownIt-Anchor" href="#asic实现"></a> Asic实现</h3>
<p><strong>FPGA 实现（基准）：</strong></p>
<p>最初的实现基于 <strong>Xilinx U280 FPGA</strong>（采用 16nm 工艺）。</p>
<p>目前以 <strong>300 MHz</strong> 的时钟频率运行。</p>
<p><strong>ASIC 预估（目标）：</strong></p>
<p>通过使用 <strong>7nm 工艺</strong>，项目团队预估该设计可以优化并达到高达 <strong>1 GHz</strong> 的时钟频率。</p>
<p><strong>芯片面积：</strong> 预估面积约为 <strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><msup><mtext>mm</mtext><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">2 \text{mm}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord"><span class="mord text"><span class="mord">mm</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></strong>。</p>
<p>这面积不到主流 Mellanox CX-7 NIC 芯片物理面积的 <strong>0.1%</strong>。</p>
<p><strong>功耗：</strong> 功耗约为 <strong>0.5 W</strong>。</p>
<p>这仅占 CX-7 NIC 热设计功耗<strong>25 W</strong> 的大约 <strong>2%</strong>。</p>
<h1 id="efficiency-of-distributed-training"><a class="markdownIt-Anchor" href="#efficiency-of-distributed-training"></a> Eﬀiciency of Distributed Training</h1>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026175746443.png" alt="image-20251026175746443" /></p>
<blockquote>
<p>一篇分析<strong>大规模分布式大语言模型训练</strong>性能、功耗与热行为的论文，研究方法感觉不需要太关注，它的结论还是比较有意思的，感觉是面向特别大规模计算中心的，估计自己这辈子都没机会用上(˘•ω•˘)</p>
</blockquote>
<h2 id="扩展策略scale-up-vs-scale-out"><a class="markdownIt-Anchor" href="#扩展策略scale-up-vs-scale-out"></a> 扩展策略（Scale-up vs. Scale-out）</h2>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026180929133.png" alt="image-20251026180929133" /></p>
<table>
<thead>
<tr>
<th><strong>模型类型</strong></th>
<th><strong>最佳硬件</strong></th>
<th><strong>优势原因</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>小且计算密集型</strong>（如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Llama3-70B</mtext><mo separator="true">,</mo><mtext>Mixtral-8x7B</mtext></mrow><annotation encoding="application/x-tex">\text{Llama3-70B}, \text{Mixtral-8x7B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">Llama3-70B</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">Mixtral-8x7B</span></span></span></span></span>）</td>
<td><strong>Scale-out</strong>（横向扩展，如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn><mo>×</mo><mtext>H100</mtext></mrow><annotation encoding="application/x-tex">64 \times \text{H100}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">6</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H100</span></span></span></span></span>）</td>
<td>具备<strong>更高的聚合计算能力</strong>。对于这些计算受限的模型，更多的 GPU 核心直接转化为更高的性能（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>H100</mtext></mrow><annotation encoding="application/x-tex">\text{H100}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H100</span></span></span></span></span> 在计算上花费的时间更少）。</td>
</tr>
<tr>
<td><strong>大且通信密集型</strong>（如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>GPT3-175B</mtext><mo separator="true">,</mo><mtext>Mixtral-8x22B</mtext></mrow><annotation encoding="application/x-tex">\text{GPT3-175B}, \text{Mixtral-8x22B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">GPT3-175B</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">Mixtral-8x22B</span></span></span></span></span>）</td>
<td><strong>Scale-up</strong>（纵向扩展，如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo>×</mo><mtext>H200</mtext></mrow><annotation encoding="application/x-tex">32 \times \text{H200}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H200</span></span></span></span></span>）</td>
<td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>H200</mtext></mrow><annotation encoding="application/x-tex">\text{H200}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H200</span></span></span></span></span> 拥有 <strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.76</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.76\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">7</span><span class="mord">6</span><span class="mord">×</span></span></span></span> 更大的显存</strong>和<strong>更少的节点</strong>。这减少了跨节点通信，提升了<strong>通信局部性（node-locality）</strong>，使其性能可以匹敌甚至超越 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>H100</mtext></mrow><annotation encoding="application/x-tex">\text{H100}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H100</span></span></span></span></span>。</td>
</tr>
</tbody>
</table>
<p>在通信量大的场景（例如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>GPT3-175B</mtext></mrow><annotation encoding="application/x-tex">\text{GPT3-175B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">GPT3-175B</span></span></span></span></span> 的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>TP2-PP16</mtext></mrow><annotation encoding="application/x-tex">\text{TP2-PP16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">TP2-PP16</span></span></span></span></span> 配置），<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>H200</mtext></mrow><annotation encoding="application/x-tex">\text{H200}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H200</span></span></span></span></span> 能够以<strong>更少的总 GPU 数量</strong>（仅 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>H100</mtext></mrow><annotation encoding="application/x-tex">\text{H100}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H100</span></span></span></span></span> 的一半）达到相当的吞吐量和更优的<strong>单位 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Token</mtext></mrow><annotation encoding="application/x-tex">\text{Token}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord text"><span class="mord">Token</span></span></span></span></span> 能耗</strong>。</p>
<blockquote>
<p>感觉是没什么意外的结果</p>
</blockquote>
<h2 id="并行策略"><a class="markdownIt-Anchor" href="#并行策略"></a> 并行策略</h2>
<h3 id="tp-pp"><a class="markdownIt-Anchor" href="#tp-pp"></a> TP + PP</h3>
<p><strong>TP所需的集合通信和PP所需的点对点通信，会交织成一种“稀疏、非连续”的通信模式</strong>。</p>
<ul>
<li><strong>缺乏数据分块</strong>：框架发出的通信操作是大量、细粒度的。例如，不是将一个大张量一次性发送，而是分解成许多小张量的Send/Recv操作。</li>
<li><strong>无法充分利用PCIe带宽</strong>：PCIe等高速总线在传输大块连续数据时效率最高。频繁、小批量的通信请求会导致：
<ul>
<li><strong>高延迟</strong>：每个小请求都有开销。</li>
<li><strong>低带宽利用率</strong>：总线在多个小任务间切换，无法“饱和”运行，就像用大卡车一次次运送小包裹，卡车大部分时间是空的。</li>
</ul>
</li>
<li><strong>加剧资源争用</strong>：这些零散的通信任务会争抢有限的PCIe通道和网络接口卡资源，进一步增加延迟。</li>
</ul>
<h3 id="pp-heavy-配置"><a class="markdownIt-Anchor" href="#pp-heavy-配置"></a> PP-heavy 配置</h3>
<p>在PP-Heavy（如<code>TP2-PP16</code>）配置中，流水线阶段很多，每个阶段内的模型切片较薄。</p>
<ul>
<li><strong>通信集中化</strong>：每个GPU只与它相邻的两个GPU（前一个阶段和后一个阶段）进行通信。通信路径非常固定和简单。</li>
<li><strong>大块数据传输</strong>：由于每个流水线阶段要处理整个微批次的激活值，在阶段间传输的<strong>数据块（Payload）更大</strong>。</li>
</ul>
<h3 id="tpep-heavy-配置"><a class="markdownIt-Anchor" href="#tpep-heavy-配置"></a> TP/EP-heavy 配置</h3>
<p><strong>节点内 vs 节点外带宽悬殊</strong>：</p>
<ul>
<li><strong>节点内</strong>：通过NVLink互联，带宽可达<strong>数百GB/s甚至TB/s</strong>。</li>
<li><strong>节点间</strong>：通过InfiniBand或以太网互联，带宽通常只有<strong>100-800 Gb/s（约合10-100 GB/s）</strong>，差了一个数量级。</li>
</ul>
<p><strong>TP/EP-Heavy放大跨节点流量</strong>：</p>
<ul>
<li>如果一个TP=8的组分布在2个节点上（每节点4个GPU），那么每次All-Reduce都有大量数据需要在相对慢速的节点间链路上传输。</li>
<li>EP的All-to-All更是如此，会制造巨大的跨节点通信洪流。<br />
<img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026185826939.png" alt="image-20251026185826939" /></li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left">并行策略</th>
<th style="text-align:left">通信模式</th>
<th style="text-align:left">主要瓶颈</th>
<th style="text-align:left">对硬件拓扑的敏感性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>TP + PP 组合</strong></td>
<td style="text-align:left">稀疏、细粒度的集合通信 + 点对点通信</td>
<td style="text-align:left">PCIe/NIC带宽<strong>利用率低下</strong>，延迟高</td>
<td style="text-align:left">高。通信模式与物理链路不匹配。</td>
</tr>
<tr>
<td style="text-align:left"><strong>PP-Heavy</strong></td>
<td style="text-align:left">集中、粗粒度的点对点通信</td>
<td style="text-align:left">流水线<strong>气泡</strong></td>
<td style="text-align:left">低。通信高效，但要注意阶段负载均衡。</td>
</tr>
<tr>
<td style="text-align:left"><strong>TP/EP-Heavy</strong></td>
<td style="text-align:left">密集、频繁的集合通信 / All-to-All</td>
<td style="text-align:left"><strong>跨节点带宽</strong> 成为天花板</td>
<td style="text-align:left">极高。必须尽可能将通信组约束在单个节点内。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>天哪，10-100 GB都嫌慢了，AI真烧钱</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026185705673.png" alt="image-20251026185705673" /></p>
</blockquote>
<h2 id="微批次大小"><a class="markdownIt-Anchor" href="#微批次大小"></a> 微批次大小</h2>
<p>增加微批次大小并非总是有效的手段 。</p>
<ul>
<li>
<p>超过最优值后，效率会降低，因为<strong>通信带宽饱和</strong>、计算回报递减以及<strong>突发执行模式</strong> 。</p>
</li>
<li>
<p>突发执行模式会提高<strong>峰值功耗</strong>和芯片温度，加剧<strong>热节流</strong>效应</p>
<h4 id="nvidia-集群h200h100"><a class="markdownIt-Anchor" href="#nvidia-集群h200h100"></a> NVIDIA 集群（H200/H100）</h4>
<ul>
<li>在**张量并行（TP）**或 <strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>FSDP</mtext></mrow><annotation encoding="application/x-tex">\text{FSDP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">FSDP</span></span></span></span></span> 主导</strong>的配置中（如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>TP8-FSDP</mtext></mrow><annotation encoding="application/x-tex">\text{TP8-FSDP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">TP8-FSDP</span></span></span></span></span>），增大 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MBS</mtext></mrow><annotation encoding="application/x-tex">\text{MBS}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MBS</span></span></span></span></span> 能有效提高计算与通信的比率，性能得到提升。</li>
<li>**** 在**流水线并行（PP）**密集型配置中（如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>TP2-PP16</mtext></mrow><annotation encoding="application/x-tex">\text{TP2-PP16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">TP2-PP16</span></span></span></span></span>），增大 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MBS</mtext></mrow><annotation encoding="application/x-tex">\text{MBS}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MBS</span></span></span></span></span> 导致效率急剧下降。
<ul>
<li><strong>原因：</strong> 流水线停顿的出现，使得执行模式更加<strong>突发</strong>，<strong>间歇性地未充分利用</strong>计算资源。</li>
</ul>
</li>
</ul>
<h4 id="2-amd-mi250-集群"><a class="markdownIt-Anchor" href="#2-amd-mi250-集群"></a> 2. AMD MI250 集群</h4>
<ul>
<li><strong>普遍提升：</strong> 对于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MI250</mtext></mrow><annotation encoding="application/x-tex">\text{MI250}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MI250</span></span></span></span></span>，增大 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MBS</mtext></mrow><annotation encoding="application/x-tex">\text{MBS}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MBS</span></span></span></span></span> 通常会提高训练效率。</li>
<li><strong>根本原因：</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MI250</mtext></mrow><annotation encoding="application/x-tex">\text{MI250}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MI250</span></span></span></span></span> 的瓶颈首先出现在<strong>内存容量</strong>上，而不是像 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>NVIDIA</mtext></mrow><annotation encoding="application/x-tex">\text{NVIDIA}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">NVIDIA</span></span></span></span></span> 集群那样容易达到<strong>热量应力</strong>的限制。</li>
<li><strong>正向循环：</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MBS</mtext></mrow><annotation encoding="application/x-tex">\text{MBS}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MBS</span></span></span></span></span> 增大 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> 工作负载更计算密集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>GPU</mtext></mrow><annotation encoding="application/x-tex">\text{GPU}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">GPU</span></span></span></span></span> 提升时钟频率 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>SM</mtext></mrow><annotation encoding="application/x-tex">\text{SM}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">SM</span></span></span></span></span>利用率和整体效率提高。</li>
</ul>
<h4 id="3-硬件压力与效率的脱钩"><a class="markdownIt-Anchor" href="#3-硬件压力与效率的脱钩"></a> 3. 硬件压力与效率的脱钩</h4>
<ul>
<li><strong>峰值压力持续上升：</strong> 无论训练吞吐量是否提高，<strong>峰值功率</strong>和<strong>热水平</strong>都随着 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MBS</mtext></mrow><annotation encoding="application/x-tex">\text{MBS}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MBS</span></span></span></span></span> 的增大而持续上升。</li>
<li>系统效应：
<ul>
<li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>TP</mtext></mrow><annotation encoding="application/x-tex">\text{TP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">TP</span></span></span></span></span> 密集配置：</strong> 峰值功率过高 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> 触发<strong>时钟频率限制</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> 降低计算效率。</li>
<li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>PP</mtext></mrow><annotation encoding="application/x-tex">\text{PP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">PP</span></span></span></span></span> 密集配置：</strong> 流水线停顿 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>突发执行模式</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> 计算资源被间歇性浪费。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026190714254.png" alt="image-20251026190714254" /></p>
<h2 id="热不平衡"><a class="markdownIt-Anchor" href="#热不平衡"></a> 热不平衡</h2>
<ul>
<li>
<p>服务器前后GPU因气流不均存在显著温差（最高达27%）。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026191805343.png" alt="image-20251026191805343" /></p>
</li>
<li>
<p>热节流导致频率下降，形成“拖后腿”GPU，影响整体同步与效率。</p>
</li>
<li>
<p>通过<strong>热感知调度</strong>（如将冷GPU分配至计算密集型阶段）可提升效率与热均衡</p>
</li>
</ul>
<blockquote>
<p>人话：让不热的GPU多干点活，温度匀一下（通风口的位置，O(∩_∩)O，莫名有点生草）</p>
</blockquote>
<h2 id="优化技术"><a class="markdownIt-Anchor" href="#优化技术"></a> 优化技术</h2>
<h3 id="激活重计算"><a class="markdownIt-Anchor" href="#激活重计算"></a> 激活重计算</h3>
<p>在模型的前向传播过程中，<strong>不保存所有中间结果</strong>，而是在反向传播需要时<strong>临时重新计算</strong>这些结果</p>
<p><strong>解锁内存受限配置</strong>：对于Mixtral-8x22B，它使得 <code>E8-T1-P4</code> 配置成为可能，从而实现了<strong>超过2倍的训练效率提升</strong>，通过牺牲计算来换取内存空间，从而<strong>扩展了可行的并行策略设计空间</strong></p>
<p><strong>普遍的性能下降</strong>，毕竟时间换空间嘛</p>
<h3 id="通信计算重叠"><a class="markdownIt-Anchor" href="#通信计算重叠"></a> 通信计算重叠</h3>
<p>将通信操作与计算操作并行执行，以“隐藏”通信延迟。</p>
<p>能够<strong>提升通信密集型场景的效率</strong>，但<strong>在PP-heavy配置中失效甚至有害</strong>：通信和计算同时进行，会争抢GPU的<strong>内存带宽</strong>和<strong>流多处理器</strong> 等共享资源。这反而可能导致<strong>计算内核的执行时间延长</strong></p>
<p>没从根本上解决有限的网络带宽。</p>
<h3 id="lora微调"><a class="markdownIt-Anchor" href="#lora微调"></a> LoRA微调</h3>
<p>冻结预训练模型的大部分权重，只训练注入到模型中的少量低秩适配器</p>
<p>由于绝大部分模型参数被冻结，无需计算梯度和更新，<strong>计算强度和内存访问强度都大幅降低</strong>,与全参数训练相比，LoRA微调期间的<strong>GPU功耗、温度和时钟频率都显著更低</strong>,可以在内存更小、功耗限制更严格的GPU上运行。</p>
<p>代价是<strong>模型表达能力可能受到限制</strong>。虽然LoRA在很多任务上表现接近全量微调，但对于某些复杂任务，其性能可能无法达到全参数训练的水平。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026192901357.png" alt="image-20251026192901357" /></p>
<blockquote>
<p>网上查到介绍原理的图，感觉这个使用还挺普遍的</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1963969812788126050">(33 封私信 / 80 条消息) 一文彻底搞懂LoRA！为什么大厂都用它微调？低成本实战指南→建议收藏！ - 知乎</a></p>
<p>感觉这个原理写的挺好的，有空可以看看</p>
</blockquote>
<h1 id="skipreduce"><a class="markdownIt-Anchor" href="#skipreduce"></a> SkipReduce</h1>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029135026682.png" alt="image-20251029135026682" /></p>
<p>在深度学习（DNN）领域，模型和数据集的规模正以前所未有的速度增长 。为了应对这种复杂性，分布式训练（即将训练任务分配给多个GPU或节点）已成为必需 。</p>
<p>然而，分布式训练引入了新的瓶颈：<strong>通信开销</strong> 。在使用数据并行时，所有“工人”（workers）在每次迭代后都需要通过 AllReduce 集合通信操作来同步梯度（gradients） 。由于训练是同步的，每个工人都必须等待通信完成才能进入下一次迭代 ，这直接导致了计算资源的浪费 和训练速度的瓶颈 。</p>
<p>在训练过程中，梯度天然是“稀疏”的 ，其数值分布集中在零附近 。这意味着并非所有梯度数据都对模型收敛至关重要 。</p>
<h2 id="现有方案的局限"><a class="markdownIt-Anchor" href="#现有方案的局限"></a> <strong>现有方案的局限：</strong></h2>
<p>此前的研究也注意到了这一点，并提出了各种压缩方法来减少通信量：</p>
<h3 id="top-k-稀疏化"><a class="markdownIt-Anchor" href="#top-k-稀疏化"></a> <strong>Top-k 稀疏化</strong></h3>
<p>只发送幅度最大的 k% 的梯度 。但这种方法需要以特殊的稀疏格式（COO）发送，这带来了额外的索引开销（导致消息翻倍） ，并且难以集成到现有的通信库（如NCCL）中 。</p>
<h3 id="powersgd"><a class="markdownIt-Anchor" href="#powersgd"></a> <strong>PowerSGD：</strong></h3>
<p>使用低秩近似来压缩梯度 。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029135934530.png" alt="image-20251029135934530" /></p>
<p>上述这些方法虽然减少了通信数据量，但引入了<strong>显著的计算开销</strong>（用于压缩、索引或重建） 。在早期的低带宽网络中，这些开销是值得的。但在现代HPC系统的高带宽（如16 GBps）环境下，<strong>这些计算开销本身成为了新的瓶颈</strong>，甚至抵消了通信节省带来的好处 。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029141455729.png" alt="image-20251029141455729" /></p>
<blockquote>
<p>SkipReduce不进行复杂的压缩，而是通过简单地“跳过” AllReduce 中的某些通信步骤，来“成批”地丢弃梯度，从而直接减少通信时间 。</p>
</blockquote>
<h2 id="skipreduce-算法实现"><a class="markdownIt-Anchor" href="#skipreduce-算法实现"></a> SkipReduce 算法实现</h2>
<table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>AllReduce_Dropout (细粒度跳过)</strong></th>
<th><strong>SkipReduce (粗粒度/切片级跳过)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>目标</strong></td>
<td>减少<strong>消息携带的信息内容</strong>（梯度信息）</td>
<td>减少<strong>通信时间</strong>（通过跳过步骤）</td>
</tr>
<tr>
<td><strong>跳过单位</strong></td>
<td><strong>单个梯度元素</strong>（基于伯努利过程）</td>
<td><strong>梯度切片</strong>（Ring AllReduce 中的一步/迭代）</td>
</tr>
<tr>
<td><strong>通信量</strong></td>
<td><strong>消息大小保持不变</strong></td>
<td><strong>通信步骤减少</strong>，直接缩短了集体操作的持续时间</td>
</tr>
<tr>
<td><strong>实现</strong></td>
<td>在归约核中对每个梯度元素进行随机决策（类似 Dropout）</td>
<td><strong>减少 Reduce-Scatter 循环的迭代次数</strong></td>
</tr>
</tbody>
</table>
<p>如果决定跳过 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span> 步 Reduce-Scatter 步骤，则 Reduce-Scatter 阶段只执行 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo>−</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N - 1 - S)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span></span></span></span> 步。</p>
<p>每跳过一步 Reduce-Scatter，就相当于<strong>跳过了每个 GPU 的一个完整的梯度切片</strong>的最终归约。</p>
<h3 id="static-skipreduce"><a class="markdownIt-Anchor" href="#static-skipreduce"></a> Static SkipReduce</h3>
<p>每次迭代中，所有 GPU <strong>跳过相同的、固定的梯度切片</strong>。</p>
<p><strong>问题：</strong> 导致<strong>偏差</strong>（或不公平性），因为相同的特定切片永远不会对集体通信结果做出贡献，可能损害模型准确性。</p>
<h3 id="random-skipreduce"><a class="markdownIt-Anchor" href="#random-skipreduce"></a> Random SkipReduce</h3>
<p>在每次训练迭代中，通过引入一个<strong>随机偏移量</strong>来移动 Reduce-Scatter 的切片索引， 确保<strong>每次迭代中跳过的切片是变化的</strong>，从而保证所有梯度切片都有机会参与到归约中，促进随机性。</p>
<p><strong>同步挑战与解决方案：</strong> 所有 GPU 必须生成相同的随机数来保证索引同步。通过修改 NCCL，将<strong>当前迭代计数</strong>作为<strong>通用种子</strong>从主机传递给所有 GPU 来解决，避免了额外的同步开销。</p>
<p>在保持几乎相同的通信加速（仅高了 1.1% 的开销）的同时，<strong>显著提高了模型准确率</strong>，其中随机SkipReduce在75%的跳过率下将测试准确率提高了19个百分点。甚至在高跳过率下能与 AllReduce_Dropout 的精度相匹配。</p>
<h3 id="selective-skipreduce"><a class="markdownIt-Anchor" href="#selective-skipreduce"></a> Selective SkipReduce</h3>
<p>梯度稀疏性在模型的不同层之间是不均匀的，如某些层中大梯度幅度的集中所示。主要结论由于梯度的幅度与其重要性相关，因此具有高浓度大梯度幅度的层可以被认为更重要，因此对跳过更敏感。相反，具有较少重要梯度的层不太敏感，是跳过的更好候选者。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029191104645.png" alt="image-20251029191104645" /></p>
<p>通过跳过大的、不重要的层，同时保护小的、关键的层以维持训练精度，从而实现显著的通信加速。</p>
<p>虽然理解为什么某些层具有相对更高的重要性可能需要严格的理论理解，并且超出了本文的范围，但某些层的重要性可以是直观的。例如，我们可以预期VGG-19中的第一个卷积层和transformers中的嵌入层对跳过更敏感，因为它们启动了输入到网络潜在空间的投影。</p>
<blockquote>
<p>有点绷不住，哈哈</p>
</blockquote>
<h2 id="结果"><a class="markdownIt-Anchor" href="#结果"></a> 结果</h2>
<h3 id="时间-精度加速"><a class="markdownIt-Anchor" href="#时间-精度加速"></a> 时间-精度加速</h3>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029192912960.png" alt="image-20251029192912960" /></p>
<p><strong>对比基线 AllReduce：</strong> 在所有工作负载和运行中，SkipReduce 都能够<strong>胜过基线 AllReduce</strong>，并实现了相对于基线 AllReduce 的中值加速。</p>
<p><strong>对比 PowerSGD：</strong> SkipReduce 在五次运行中实现了相对于 PowerSGD <strong>平均 16% 的 TTA 加速</strong>。</p>
<h3 id="迭代时间对比"><a class="markdownIt-Anchor" href="#迭代时间对比"></a> 迭代时间对比</h3>
<p>在 LLaMA3.2 上，SkipReduce 的迭代时间比 PowerSGD <strong>慢 6%</strong></p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029193046136.png" alt="image-20251029193046136" /></p>
<h3 id="跳过-reduce-scatter-rs-与-allgather-ag-的权衡"><a class="markdownIt-Anchor" href="#跳过-reduce-scatter-rs-与-allgather-ag-的权衡"></a> 跳过 Reduce-Scatter (RS) 与 AllGather (AG) 的权衡</h3>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029193359456.png" alt="image-20251029193359456" /></p>
<table>
<thead>
<tr>
<th><strong>方案</strong></th>
<th><strong>跳过步骤</strong></th>
<th><strong>性能影响</strong></th>
<th><strong>准确率损失</strong></th>
<th><strong>关键发现</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>方案一 (SkipReduce 现有)</strong></td>
<td>跳过 <strong>50% 的 Reduce-Scatter (RS)</strong></td>
<td><strong>收敛速度快</strong></td>
<td>相对较小</td>
<td>仅针对 RS 阶段可以减少通信时间。</td>
</tr>
<tr>
<td><strong>方案二 (扩展 SkipReduce)</strong></td>
<td>跳过 <strong>25% 的 RS 和 25% 的 AllGather (AG)</strong></td>
<td><strong>显著降低收敛速度</strong></td>
<td>训练准确率降低 <strong>1.95</strong> 个点；测试准确率降低 <strong>1.3</strong> 个点。</td>
<td><strong>跳过 AllGather 的代价更高</strong>，因为它涉及所有其他 GPU 的信息，导致更高的信息损失。</td>
</tr>
</tbody>
</table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://iczjn.com">ZJN</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://iczjn.com/posts/1253395412.html">http://iczjn.com/posts/1253395412.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://iczjn.com" target="_blank">Cosmos</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文</a><a class="post-meta__tags" href="/tags/MICRO2025/">MICRO2025</a></div><div class="post-share"><div class="social-share" data-image="/img/cover_16.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/3740239699.html" title="Neural_MP论文阅读"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_12.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Neural_MP论文阅读</div></div><div class="info-2"><div class="info-item-1">  概述  论文概述 通过大规模数据驱动的学习，构建一个通用、快速运动规划器，能泛化到未见过的真实世界场景。  成果 提出了一种简单、可扩展的方法来训练和部署快速、通用的神经运动规划器：1) 在逼真的配置中生成具有多样化环境的大规模程序化场景，2) 用于拟合基于采样的运动规划数据的多模态序列建模，以及 3) 轻量级的测试时优化，以确保在现实世界中快速、安全和可靠的部署。 在四个不同的环境中评估了我们的方法在 64 个现实世界运动规划任务中的表现，结果表明，与基于采样的方法相比，运动规划成功率提高了 23%，与基于优化的方法相比提高了 17%，与神经运动规划方法相比提高了 79%。         实现  生成大规模训练数据 ...</div></div></div></a><a class="pagination-related" href="/posts/2274654979.html" title="Window_use环境配置和UI编写"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_15.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Window_use环境配置和UI编写</div></div><div class="info-2"><div class="info-item-1">原项目地址：https://github.com/CursorTouch/Windows-Use.git 有趣的玩具，自己写了个小ui，他llm跟gui层的Windows接起来还是很有意思的，允许AI直接在GUI层面与Windows操作系统交互。但用起来确实笨笨的，稍微复杂一点就不行了，只能当个玩具了，不过他内层还有不少工具，应该能更深入地开发。 因为国外的API充值要国外的银行卡，我把环境替换成了使用qwen。  效果        ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/3740229496.html" title="AXI总线探索"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_2.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-28</div><div class="info-item-2">AXI总线探索</div></div><div class="info-2"><div class="info-item-1"> 前言 最近需要想办法给CPU接AXI总线，目前先试着看下AXI4-LITE，IFU和ROM已经写好了，开始往LSU和RAM看看  AXI4-lite  内部握手信号 VALID信号是传输段发送的，READY是接收端发送的。只有在VALID和READY同时有效的时候，信息才会被传递。VALID有效代表发送以准备好，READY有效代表接收已准备好，当其中一方有效，另一方无效时，有效的一方信号需要保持，等待另一方信号的有效。 写响应通道要发送BVALID和BRESP。其中BVALID要与主机发过来的BREADY进行握手代表写数据结束。BRESP为两个bit的信号，作为写数据是否成功的回应 写模式  读模式  LSU伪代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class AXILSU(params: AXI4Params) extends Module &#123;  // AXI4接口信号（简化版）  val io =...</div></div></div></a><a class="pagination-related" href="/posts/3740229498.html" title="BOOM源码与《超标量处理器设计》学习"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_11.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-18</div><div class="info-item-2">BOOM源码与《超标量处理器设计》学习</div></div><div class="info-2"><div class="info-item-1"> BOOM  寄存器重命名  RenameStage - 重命名阶段顶层模块 RenameMapTable - 寄存器映射表 RenameFreeList - 空闲物理寄存器列表 RenameBusyTable - 寄存器忙状态表   RenameStage  整体两阶段流水线：   阶段1(Ren1)：接收来自解码阶段的微操作(uop)   阶段2(Ren2)：完成重命名并发送到调度队列   通过MapTable完成逻辑寄存器到物理寄存器的映射   通过FreeList分配新的物理寄存器   通过BusyTable跟踪寄存器忙状态   处理分支预测错误时的恢复    AbstractRenameStage——寄存器重命名的基础抽象类 micro微操作集定义在micro-op.scala 包含了指令基本信息，逻辑寄存器编号，物理寄存器编号（重命名阶段分配），寄存器忙状态（BusyTable 提供），分支预测相关，执行信息  1....</div></div></div></a><a class="pagination-related" href="/posts/3741229496.html" title="riscv扩展指令集"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_13.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-26</div><div class="info-item-2">riscv扩展指令集</div></div><div class="info-2"><div class="info-item-1">看了WAIC，感觉这个真有意思   Gemini 扩展  硬件 Gemmini的空间阵列用于执行MAC操作，空间阵列由Scratchpad SRAM读取数据，并将输出结果写入Accumulator SRAM；同时还支持其他常见的DNN计算核（如非线性激活、池化等）。 脉动阵列（Systolic Array）：执行矩阵乘法 显式管理的内存：Scratchpad（输入缓存） + Accumulator（累加器） DMA引擎：主存与私有SRAM间的数据传输 解耦访问/执行：Load/Store/Execute三控制器并行工作  Gemmini的空间阵列由Tile、PE两级架构组成，Tile通过显式流水线寄存器相连接；而Tile可进一步细分为PE，同一Tile中的PE通过组合电路连接，而不使用流水线寄存器。每个PE每个周期执行一次MAC运算，使用WS或OS的数据流形式。每个PE和每个Tile只与其相邻的单元共享输入和输出。    ISA  数据移动指令  mvin - 从主内存加载数据到暂存器  格式：mvin rs1,...</div></div></div></a><a class="pagination-related" href="/posts/3521918645.html" title="适配AXI的地址对齐"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_8.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-11</div><div class="info-item-2">适配AXI的地址对齐</div></div><div class="info-2"><div class="info-item-1"> AXI4引入的arsize/awsize 完整的AXI总线协议通过arsize/awsize信号来指示实际访问的数据位宽, 同时引入&quot;窄传输&quot;的概念, 用于指示&quot;实际数据位宽小于总线数据位宽&quot;的情况. 这两个&quot;数据位宽&quot;的概念并非完全一致, 具体地, 总线数据位宽是在硬件设计时静态决定的, 它表示一次总线传输的最大数据位宽, 也用于计算总线的理论带宽; 而实际数据位宽(即arsize/awsize信号的值)则由软件访存指令中的位宽信息动态决定, 它表示一次总线传输的实际数据位宽, 例如, lb指令只访问1字节, 而lw指令则访问4字节。 也就是AXI总线会根据我们设置的arsize/awsize来映射对齐地址，设置为0时，会对齐1b,设置为1时，会对齐2b,设置为4时，会对齐4b。  strb选通信号与data  S指令有效数据与地址的对齐 wstrb与地址相适配，也要将有效数据移至有效字节，以sb指令为例 123456789101112is(EXE_SB_OP) &#123;  mem_addr :=...</div></div></div></a><a class="pagination-related" href="/posts/132441525.html" title="ld链接与简易bootloader（添加全局变量支持）"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_7.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-08</div><div class="info-item-2">ld链接与简易bootloader（添加全局变量支持）</div></div><div class="info-2"><div class="info-item-1"> 堆区 在启动文件里分配，作为用户主动申请时的空间，如调用malloc()  栈区 在启动文件里分配，作为局部变量自动申请和释放空间的变量（也有说是编译器分配的空间）  bss 存放未初始化的全局变量和静态变量；  data 存放初始化后的全局变量和静态变量；  已初始化的全局变量（data）嵌入程序，不可能直接存在RAM中，必须先载入ROM，再通过memcpy载入RAM中（这也是数电H中我没能想到的，不过那时也没实现标准库也不太可能搞这种骚操作），bss由于是未初始化的，直接链接到RAM就行。 12345678910111213141516 _sidata = LOADADDR(.data); .data : &#123;   _sdata = .;   *(.data*)   *(.sdata*)   _edata = .; &#125; &gt; sram AT &gt;mrom :data.bss :  &#123;     _bstart = .;   *(.bss*)   *(.sbss*)   *(.scommon)   _bend = .; &#125;...</div></div></div></a><a class="pagination-related" href="/posts/3740239749.html" title="JanusVLN论文阅读"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_19.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-28</div><div class="info-item-2">JanusVLN论文阅读</div></div><div class="info-2"><div class="info-item-1"> 高德和西交最近的论文，这几天要开源    概括 JanusVLN具有双重隐式神经记忆，将空间几何记忆和视觉语义记忆建模为独立的、紧凑的、固定大小的神经表征。该框架首先扩展了 MLLM，使其能够整合来自空间几何编码器的 3D 先验知识，从而增强了仅基于 RGB 输入的模型的空间推理能力。然后，将来自空间几何和视觉语义编码器的历史键值（KV）缓存构建成双重隐式记忆。通过仅保留初始和滑动窗口中令牌的 KV，避免了冗余计算，从而实现了高效的增量更新。         与先前研究的对比  VLN方法局限 当前主流的基于MLLM的VLN方法普遍依赖显式记忆机制（如文本拓扑地图、历史图像序列）： 语义认知地图： 使用纯文本描述对象节点和关系边（MapNav，后面可以看下）。 存储历史观测帧：...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "/img/loading.gif" data-lazy-src="/img/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">ZJN</div><div class="author-info-description">穷其道者 归处亦同</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">15</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/FDU-ZJN"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">嘟嘟噜</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#netzlp"><span class="toc-number">1.</span> <span class="toc-text"> NetZlP</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%BC%8F%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94%E8%A1%A8"><span class="toc-number">1.0.1.</span> <span class="toc-text"> 并行训练模式总结对比表</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.</span> <span class="toc-text"> 压缩算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BA%A7%E5%88%AB%E8%BD%AC%E6%8D%A2%E5%AD%97%E8%8A%82%E5%92%8C%E6%AF%94%E7%89%B9%E5%88%86%E7%BB%84"><span class="toc-number">1.1.1.</span> <span class="toc-text"> 位级别转换：字节和比特分组</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%80%BC%E7%BA%A7%E5%88%AB%E8%BD%AC%E6%8D%A2delta-%E5%8E%8B%E7%BC%A9"><span class="toc-number">1.1.2.</span> <span class="toc-text"> 值级别转换：Delta 压缩</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E5%8A%A0%E9%80%9F%E5%99%A8"><span class="toc-number">1.2.</span> <span class="toc-text"> 压缩加速器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E8%B7%AF%E5%BE%84"><span class="toc-number">1.2.1.</span> <span class="toc-text"> 压缩路径</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%8E%8B%E7%BC%A9%E8%B7%AF%E5%BE%84decompression-path"><span class="toc-number">1.2.2.</span> <span class="toc-text"> 解压缩路径（Decompression Path）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#asic%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.2.3.</span> <span class="toc-text"> Asic实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#efficiency-of-distributed-training"><span class="toc-number">2.</span> <span class="toc-text"> Eﬀiciency of Distributed Training</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A9%E5%B1%95%E7%AD%96%E7%95%A5scale-up-vs-scale-out"><span class="toc-number">2.1.</span> <span class="toc-text"> 扩展策略（Scale-up vs. Scale-out）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5"><span class="toc-number">2.2.</span> <span class="toc-text"> 并行策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tp-pp"><span class="toc-number">2.2.1.</span> <span class="toc-text"> TP + PP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pp-heavy-%E9%85%8D%E7%BD%AE"><span class="toc-number">2.2.2.</span> <span class="toc-text"> PP-heavy 配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tpep-heavy-%E9%85%8D%E7%BD%AE"><span class="toc-number">2.2.3.</span> <span class="toc-text"> TP&#x2F;EP-heavy 配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E6%89%B9%E6%AC%A1%E5%A4%A7%E5%B0%8F"><span class="toc-number">2.3.</span> <span class="toc-text"> 微批次大小</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#nvidia-%E9%9B%86%E7%BE%A4h200h100"><span class="toc-number">2.3.0.1.</span> <span class="toc-text"> NVIDIA 集群（H200&#x2F;H100）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-amd-mi250-%E9%9B%86%E7%BE%A4"><span class="toc-number">2.3.0.2.</span> <span class="toc-text"> 2. AMD MI250 集群</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%A1%AC%E4%BB%B6%E5%8E%8B%E5%8A%9B%E4%B8%8E%E6%95%88%E7%8E%87%E7%9A%84%E8%84%B1%E9%92%A9"><span class="toc-number">2.3.0.3.</span> <span class="toc-text"> 3. 硬件压力与效率的脱钩</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%83%AD%E4%B8%8D%E5%B9%B3%E8%A1%A1"><span class="toc-number">2.4.</span> <span class="toc-text"> 热不平衡</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-number">2.5.</span> <span class="toc-text"> 优化技术</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E9%87%8D%E8%AE%A1%E7%AE%97"><span class="toc-number">2.5.1.</span> <span class="toc-text"> 激活重计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E4%BF%A1%E8%AE%A1%E7%AE%97%E9%87%8D%E5%8F%A0"><span class="toc-number">2.5.2.</span> <span class="toc-text"> 通信计算重叠</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lora%E5%BE%AE%E8%B0%83"><span class="toc-number">2.5.3.</span> <span class="toc-text"> LoRA微调</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#skipreduce"><span class="toc-number">3.</span> <span class="toc-text"> SkipReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%B0%E6%9C%89%E6%96%B9%E6%A1%88%E7%9A%84%E5%B1%80%E9%99%90"><span class="toc-number">3.1.</span> <span class="toc-text"> 现有方案的局限：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#top-k-%E7%A8%80%E7%96%8F%E5%8C%96"><span class="toc-number">3.1.1.</span> <span class="toc-text"> Top-k 稀疏化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#powersgd"><span class="toc-number">3.1.2.</span> <span class="toc-text"> PowerSGD：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#skipreduce-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.2.</span> <span class="toc-text"> SkipReduce 算法实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#static-skipreduce"><span class="toc-number">3.2.1.</span> <span class="toc-text"> Static SkipReduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#random-skipreduce"><span class="toc-number">3.2.2.</span> <span class="toc-text"> Random SkipReduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#selective-skipreduce"><span class="toc-number">3.2.3.</span> <span class="toc-text"> Selective SkipReduce</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C"><span class="toc-number">3.3.</span> <span class="toc-text"> 结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%B6%E9%97%B4-%E7%B2%BE%E5%BA%A6%E5%8A%A0%E9%80%9F"><span class="toc-number">3.3.1.</span> <span class="toc-text"> 时间-精度加速</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%AD%E4%BB%A3%E6%97%B6%E9%97%B4%E5%AF%B9%E6%AF%94"><span class="toc-number">3.3.2.</span> <span class="toc-text"> 迭代时间对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%B3%E8%BF%87-reduce-scatter-rs-%E4%B8%8E-allgather-ag-%E7%9A%84%E6%9D%83%E8%A1%A1"><span class="toc-number">3.3.3.</span> <span class="toc-text"> 跳过 Reduce-Scatter (RS) 与 AllGather (AG) 的权衡</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/3740239749.html" title="JanusVLN论文阅读"><img src= "/img/loading.gif" data-lazy-src="/img/cover_19.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="JanusVLN论文阅读"/></a><div class="content"><a class="title" href="/posts/3740239749.html" title="JanusVLN论文阅读">JanusVLN论文阅读</a><time datetime="2025-10-28T03:20:30.000Z" title="发表于 2025-10-28 11:20:30">2025-10-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/2274654979.html" title="Window_use环境配置和UI编写"><img src= "/img/loading.gif" data-lazy-src="/img/cover_15.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Window_use环境配置和UI编写"/></a><div class="content"><a class="title" href="/posts/2274654979.html" title="Window_use环境配置和UI编写">Window_use环境配置和UI编写</a><time datetime="2025-10-25T06:46:07.000Z" title="发表于 2025-10-25 14:46:07">2025-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/1253395412.html" title="MICRO 2025 Systems for Al (Training) 部分论文阅读"><img src= "/img/loading.gif" data-lazy-src="/img/cover_16.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MICRO 2025 Systems for Al (Training) 部分论文阅读"/></a><div class="content"><a class="title" href="/posts/1253395412.html" title="MICRO 2025 Systems for Al (Training) 部分论文阅读">MICRO 2025 Systems for Al (Training) 部分论文阅读</a><time datetime="2025-10-22T00:04:13.000Z" title="发表于 2025-10-22 08:04:13">2025-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3740239699.html" title="Neural_MP论文阅读"><img src= "/img/loading.gif" data-lazy-src="/img/cover_12.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Neural_MP论文阅读"/></a><div class="content"><a class="title" href="/posts/3740239699.html" title="Neural_MP论文阅读">Neural_MP论文阅读</a><time datetime="2025-10-20T00:35:17.000Z" title="发表于 2025-10-20 08:35:17">2025-10-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3740239499.html" title="π0阅读复现"><img src= "/img/loading.gif" data-lazy-src="/img/cover_14.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="π0阅读复现"/></a><div class="content"><a class="title" href="/posts/3740239499.html" title="π0阅读复现">π0阅读复现</a><time datetime="2025-10-18T02:38:40.000Z" title="发表于 2025-10-18 10:38:40">2025-10-18</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2025 By ZJN</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax src="https://unpkg.com/oh-my-live2d"></script><script>const oml2d = OML2D.loadOml2d({importType:"cubism5",dockedPosition:"right",mobileDisplay:false,models:[{"path":"/live2d_models/ugofficial/ugofficial.model3.json","position":[0,0],"scale":0.25,"stageStyle":{"width":200,"height":230},"mobilePosition":[-20,-10],"mobileScale":0.06,"mobileStageStyle":{"width":150,"height":140},"motionPreloadStrategy":"ALL"}],tips:{style: {"width":200,"height":70,"left":"calc(50% - 20px)","top":"-100px"},mobileStyle: {"width":150,"left":10,"bottom":10,"top":"auto"},idleTips:{interval:3600,message:["BUG DE完了吗~"]}}});</script><!-- hexo injector body_end end --></body></html>