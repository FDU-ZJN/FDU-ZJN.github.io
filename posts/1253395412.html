<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>MICRO2025论文阅读 | Cosmos</title><meta name="author" content="ZJN"><meta name="copyright" content="ZJN"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="MICRO 2025 Systems for Al (Training) 部分的论文阅读，论文内容集中在分布式训练有关的通信">
<meta property="og:type" content="article">
<meta property="og:title" content="MICRO2025论文阅读">
<meta property="og:url" content="http://iczjn.com/posts/1253395412.html">
<meta property="og:site_name" content="Cosmos">
<meta property="og:description" content="MICRO 2025 Systems for Al (Training) 部分的论文阅读，论文内容集中在分布式训练有关的通信">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://iczjn.com/img/cover_16.jpg">
<meta property="article:published_time" content="2025-10-22T00:04:13.000Z">
<meta property="article:modified_time" content="2025-10-31T06:20:23.318Z">
<meta property="article:author" content="ZJN">
<meta property="article:tag" content="论文">
<meta property="article:tag" content="体系结构">
<meta property="article:tag" content="MICRO">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://iczjn.com/img/cover_16.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "MICRO2025论文阅读",
  "url": "http://iczjn.com/posts/1253395412.html",
  "image": "http://iczjn.com/img/cover_16.jpg",
  "datePublished": "2025-10-22T00:04:13.000Z",
  "dateModified": "2025-10-31T06:20:23.318Z",
  "author": [
    {
      "@type": "Person",
      "name": "ZJN",
      "url": "http://iczjn.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://iczjn.com/posts/1253395412.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'MICRO2025论文阅读',
  isHighlightShrink: true,
  isToc: true,
  pageType: 'post'
}</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script async="async">(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)
  setTimeout(function(){preloader.endLoading();}, 3000);

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="web_bg" style="background-image: url(/img/background.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "/img/loading.gif" data-lazy-src="/img/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/cover_16.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Cosmos</span></a><a class="nav-page-title" href="/"><span class="site-name">MICRO2025论文阅读</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">MICRO2025论文阅读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-22T00:04:13.000Z" title="发表于 2025-10-22 08:04:13">2025-10-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-31T06:20:23.318Z" title="更新于 2025-10-31 14:20:23">2025-10-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/IC/">IC</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h3 id="分布式训练策略"><a class="markdownIt-Anchor" href="#分布式训练策略"></a> 分布式训练策略</h3>
<table>
<thead>
<tr>
<th style="text-align:left">维度</th>
<th style="text-align:left">数据并行 (DP)</th>
<th style="text-align:left">流水线并行 (PP)</th>
<th style="text-align:left">张量并行 (TP)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>并行核心思想</strong></td>
<td style="text-align:left"><strong>数据维度</strong>：将训练数据分批，在不同的设备上使用<strong>相同的模型副本</strong>进行处理。</td>
<td style="text-align:left"><strong>模型维度（层间）</strong>：将模型按<strong>层</strong>拆分成多个阶段，每个设备负责模型的一个连续部分。</td>
<td style="text-align:left"><strong>模型维度（层内）</strong>：将单个<strong>层内的运算和参数</strong>进行拆分，分布到多个设备上。</td>
</tr>
<tr>
<td style="text-align:left"><strong>如何划分</strong></td>
<td style="text-align:left">划分<strong>训练数据集</strong>。</td>
<td style="text-align:left">划分<strong>模型的层</strong>。</td>
<td style="text-align:left">划分<strong>层的权重矩阵/计算</strong>。</td>
</tr>
<tr>
<td style="text-align:left"><strong>设备上的模型状态</strong></td>
<td style="text-align:left">每个设备拥有<strong>完整模型</strong>的一个副本。</td>
<td style="text-align:left">每个设备只拥有<strong>模型的一部分</strong>（一组连续的层）。</td>
<td style="text-align:left">每个设备拥有<strong>一层或几层的部分参数</strong>。</td>
</tr>
<tr>
<td style="text-align:left"><strong>通信内容</strong></td>
<td style="text-align:left"><strong>梯度</strong>（反向传播后）</td>
<td style="text-align:left"><strong>激活值</strong>（前向传播时） <strong>梯度</strong>（反向传播时）</td>
<td style="text-align:left"><strong>部分激活值/计算结果</strong>（前向和反向传播过程中）</td>
</tr>
<tr>
<td style="text-align:left"><strong>通信时机</strong></td>
<td style="text-align:left">每次迭代的<strong>反向传播结束后</strong>。</td>
<td style="text-align:left">在前向和反向传播过程中，<strong>阶段与阶段之间</strong>。</td>
<td style="text-align:left">在<strong>单个层的前向和反向计算过程中</strong>。</td>
</tr>
<tr>
<td style="text-align:left"><strong>主要优势</strong></td>
<td style="text-align:left">- 实现简单，应用广泛。 - 对于模型较小、数据量大的情况非常有效。</td>
<td style="text-align:left">- 可以训练<strong>模型过大，无法放入单个设备显存</strong>的模型。 - 通信量相对可控。</td>
<td style="text-align:left">- 能极细粒度地降低<strong>单个巨大层</strong>的内存占用（如大模型中的FFN、Attention层）。</td>
</tr>
<tr>
<td style="text-align:left"><strong>主要挑战/代价</strong></td>
<td style="text-align:left">- 每个设备仍需容纳<strong>整个模型</strong>，对于大模型不适用。 - 全局同步梯度会产生通信瓶颈，尤其是设备数量多时。</td>
<td style="text-align:left">- 存在<strong>流水线气泡</strong>，造成设备空闲，降低计算效率。 - 需要仔细进行模型切分以平衡各阶段负载</td>
<td style="text-align:left">- <strong>通信非常频繁</strong>（每层都可能需要通信），对设备间互联带宽要求极高。</td>
</tr>
</tbody>
</table>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1937826285264011929">(33 封私信 / 80 条消息) 一文深度全面解析大模型分布式并行策略：DP/TP/PP/CP/EP/SP - 知乎</a></p>
<p>一个讲训练模式的文章，觉得讲的不错。</p>
<table>
<thead>
<tr>
<th style="text-align:left">并行策略</th>
<th style="text-align:left">缩写</th>
<th style="text-align:left">核心通信</th>
<th style="text-align:left">通信发生时机与目的</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>数据并行</strong></td>
<td style="text-align:left">Data Parallelism (<strong>DP</strong>)</td>
<td style="text-align:left"><strong>All-Reduce</strong></td>
<td style="text-align:left">反向传播结束后，同步所有设备间的梯度。</td>
</tr>
<tr>
<td style="text-align:left"><strong>流水线并行</strong></td>
<td style="text-align:left">Pipeline Parallelism (<strong>PP</strong>)</td>
<td style="text-align:left"><strong>点对点通信</strong> (Peer-to-Peer)</td>
<td style="text-align:left">在流水线相邻阶段之间传递激活值（前向）和梯度（反向）。</td>
</tr>
<tr>
<td style="text-align:left"><strong>张量并行</strong></td>
<td style="text-align:left">Tensor Parallelism (<strong>TP</strong>)</td>
<td style="text-align:left"><strong>All-to-All</strong></td>
<td style="text-align:left">在前向/反向传播的<strong>计算过程中</strong>，交换和重组中间激活值。</td>
</tr>
<tr>
<td style="text-align:left"><strong>序列并行</strong></td>
<td style="text-align:left">Sequence Parallelism (<strong>SP</strong>)</td>
<td style="text-align:left"><strong>All-to-All</strong> / <strong>All-Gather</strong></td>
<td style="text-align:left">在处理序列维度时，重组与序列维度相关的激活值。</td>
</tr>
<tr>
<td style="text-align:left"><strong>专家并行</strong></td>
<td style="text-align:left">Expert Parallelism (<strong>EP</strong>)</td>
<td style="text-align:left"><strong>All-to-All</strong></td>
<td style="text-align:left">在混合专家模型中，将数据路由到不同设备上</td>
</tr>
</tbody>
</table>
</blockquote>
<h1 id="netzlp"><a class="markdownIt-Anchor" href="#netzlp"></a> NetZlP</h1>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251022081259452.png" alt="image-20251022081259452" /></p>
<blockquote>
<p>在分布式大模型训练中实现<strong>网络内无损压缩</strong>梯度和激活,实现了压缩算法和硬件加速器</p>
</blockquote>
<p>顶级超大规模集群将其每个计算节点连接到多个400 Gbps网络链路[13, 49]。然而，公共云中经济实惠的集群仍然依赖于中低带宽的网络链路。范围从低至10 Gbps到常见的50 Gbps。</p>
<p>并非所有人都能轻易负担得起访问网络带宽超过100 Gbps的高端实例。当网络带宽降低到50 Gbps时，Llama-3 70B、GPT-3 175B和Llama-3 405B的每次训练迭代的通信时间分别增加2.2×、2.0×和1.7×。</p>
<h2 id="压缩算法"><a class="markdownIt-Anchor" href="#压缩算法"></a> 压缩算法</h2>
<p>在分布式大模型训练中，梯度和激活通常采用 <strong>bfloat16</strong> 格式存储（16位浮点数）。标准无损压缩算法（如 LZ4 和 Snappy）主要依赖于<strong>字典匹配</strong>，通过滑动窗口查找重复的字符（8位字节序列）来压缩数据 。</p>
<p>bfloat16 数据的 <strong>低 8 位</strong> 接近随机分布（1的概率约为 50%），这使得标准压缩算法很难找到重复的字节序列 。实验显示，LZ4 和 Snappy 几乎无法压缩 bfloat16 格式的梯度和激活（压缩比接近 100%）。</p>
<table>
<thead>
<tr>
<th style="text-align:left">格式</th>
<th style="text-align:left">总位数</th>
<th style="text-align:left">符号位</th>
<th style="text-align:left">指数位</th>
<th style="text-align:left">尾数位</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>FP32 (float)</strong></td>
<td style="text-align:left">32 bits</td>
<td style="text-align:left">1 bit</td>
<td style="text-align:left">8 bits</td>
<td style="text-align:left">23 bits</td>
</tr>
<tr>
<td style="text-align:left"><strong>FP16 (half)</strong></td>
<td style="text-align:left">16 bits</td>
<td style="text-align:left">1 bit</td>
<td style="text-align:left">5 bits</td>
<td style="text-align:left">10 bits</td>
</tr>
<tr>
<td style="text-align:left"><strong>bfloat16</strong></td>
<td style="text-align:left">16 bits</td>
<td style="text-align:left">1 bit</td>
<td style="text-align:left">8 bits</td>
<td style="text-align:left">7 bits</td>
</tr>
</tbody>
</table>
<h3 id="位级别转换字节和比特分组"><a class="markdownIt-Anchor" href="#位级别转换字节和比特分组"></a> 位级别转换：字节和比特分组</h3>
<p>虽然尾数位是随机的，但梯度和激活的 <strong>指数位</strong> 具有较高的“1”的概率（1.07169×10−8 = 0_01100100_0111000) 。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251027090838491.png" alt="image-20251027090838491" /></p>
<p><strong>字节分组：</strong> 将所有 bfloat16 数值中的<strong>高字节</strong>（包含符号位和指数位）集中在一起，再将所有<strong>低字节</strong>（包含尾数位）集中在一起 7。</p>
<p><strong>比特分组：</strong> 将所有 bfloat16 数值中的<strong>第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 个比特</strong>（例如，所有数的第 15 位）集中在一起，依此类推 。</p>
<p>通过分组，可压缩性更高的指数位和符号位被排列到相邻位置，从而帮助 LZ4 等算法更容易识别重复模式，大幅提高字典匹配的效率 。</p>
<h3 id="值级别转换delta-压缩"><a class="markdownIt-Anchor" href="#值级别转换delta-压缩"></a> 值级别转换：Delta 压缩</h3>
<p>梯度和激活的值在训练迭代过程中是<strong>逐渐变化</strong>的 。</p>
<p>从当前迭代的每个梯度或激活值中，减去一个<strong>有代表性的基准值</strong> ：</p>
<ul>
<li>理想情况下，可以减去上一迭代的对应值（称为“真 Delta 压缩”）。</li>
<li>在 NIC 内存受限的实际场景中，论文提出为每一层使用一个<strong>单基准值</strong>，例如该层中所有值的<strong>最小值</strong> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>) 。</li>
</ul>
<p>进行减法操作后得到的<strong>Delta 值</strong>会更集中于零点附近，这使得原本随机的<strong>尾数位更趋向于 0</strong>，从而使数据整体更具可压缩性 。</p>
<p>实验证明，<strong>Delta 压缩结合比特分组</strong>辅助下的 NETZIP-LZ4，相比标准 LZ4 实现了巨大的提升 ：</p>
<p>梯度的数据量平均减少了 <strong>67%</strong> 。</p>
<p>激活的数据量平均减少了 <strong>70%</strong> 。</p>
<h2 id="压缩加速器"><a class="markdownIt-Anchor" href="#压缩加速器"></a> 压缩加速器</h2>
<p>提出了NetZIP-accelerator加速器，集成到NIC ASIC中。</p>
<p>NETZIP-accelerator 采用了<strong>线内</strong>的架构 ，意味着它将整个处理模块直接放置在数据流向网络（或从网络接收）的路径上。</p>
<p><strong>集成位置：</strong> 加速器将 NETZIP-algorithm 的转换功能与一个<strong>LZ4加速器</strong>集成到 NIC（网卡）内部 。</p>
<p><strong>解决传输开销：</strong> 传统的压缩方式（在 CPU、GPU 或 SNIC 上运行）需要额外的内存传输 。例如，数据在发送前必须经历：发送方 GPU 内存 -&gt; CPU 内存 -&gt; CPU 压缩 -&gt; CPU 内存 -&gt; NIC 的路径 。这些<strong>额外的传输</strong>显著增加了端到端的压缩/解压缩延迟 。</p>
<p>作为“线内”加速器，NETZIP-accelerator <strong>消除了</strong>这些不必要的梯度和激活数据的<strong>额外传输</strong> ，从而直接在数据流经 NIC 时完成转换、压缩和解压缩，大幅减少了总体延迟 。</p>
<h3 id="压缩路径"><a class="markdownIt-Anchor" href="#压缩路径"></a> 压缩路径</h3>
<p>当数据从主机（GPU 内存）通过 NIC 发送到网络时，会经过压缩路径：</p>
<p><strong>组件顺序：</strong> <strong>Tx (DE)MUX</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>Delta 编码器</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>分组暂存缓冲区</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>分组器</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>压缩器</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>Tx MUX/协议引擎</strong></p>
<table>
<thead>
<tr>
<th><strong>组件</strong></th>
<th><strong>功能</strong></th>
<th><strong>机制/实现细节</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Tx (DE)MUX</strong></td>
<td><strong>流量控制/选择性压缩</strong></td>
<td>根据自定义标头中的 <strong>1 位压缩标志</strong>，决定数据包的去向： 1. 如果标志未设置（非梯度/激活数据包），直接将数据包转发给协议引擎（不压缩）。 2. 如果标志设置，将数据包导向 Delta 编码器进行压缩。</td>
</tr>
<tr>
<td><strong>Delta 编码器</strong></td>
<td><strong>值级别转换</strong></td>
<td>负责执行 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>i</mi></msub><mo>−</mo><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">G_{i} - D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 操作，由三个单元组成： 1. <strong>基值计算单元：</strong> 遍历数据包中<strong>同一层 ID</strong>（由 15 位 Layer ID 标识）的 bfloat16 值，找出该层的<strong>最小值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></strong> 作为基值。 2. <strong>基值寄存器：</strong> 存储每一层的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 基值。 3. <strong>增量值计算单元：</strong> 计算每个中间值（梯度或激活）与 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的差值，生成 <strong>Delta 值</strong>。计算完成后，将 Delta 值和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 一起传出。</td>
</tr>
<tr>
<td><strong>分组暂存缓冲</strong></td>
<td><strong>缓存与双缓冲</strong></td>
<td>采用<strong>双缓冲架构</strong>： 1. 当一个缓冲区在<strong>接收</strong>当前 4 kB 数据包时。 2. 另一个缓冲区则存储<strong>前一个</strong>已完成 Delta 编码的数据包，供分组器使用。此设计确保了数据流的连续性，最小化延迟。</td>
</tr>
<tr>
<td><strong>分组器 (Grouper)</strong></td>
<td><strong>位级别转换</strong></td>
<td>对 Delta 值执行 <strong>字节或比特粒度</strong> 的重排列（即 NETZIP-algorithm 中的<strong>比特/字节分组</strong>），将可压缩的部分聚集在一起。</td>
</tr>
<tr>
<td><strong>压缩器 (Compressor)</strong></td>
<td><strong>无损压缩</strong></td>
<td>对分组后的 Delta 值进行 <strong>LZ4 无损压缩</strong>。压缩完成后，将<strong>基值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 附加到压缩后的有效载荷末尾</strong>（用于解压缩时的恢复）。  <strong>故障处理：</strong> 如果压缩后的大小仍超过 4 kB，则直接丢弃压缩结果，将<strong>原始未压缩数据包</strong>（移除自定义标头后）发送给协议引擎。</td>
</tr>
<tr>
<td><strong>Tx MUX</strong></td>
<td><strong>出口选择</strong></td>
<td>将压缩后的数据包（或在压缩失败时将原始数据包）发送给协议引擎，准备发往网络。</td>
</tr>
</tbody>
</table>
<h3 id="解压缩路径decompression-path"><a class="markdownIt-Anchor" href="#解压缩路径decompression-path"></a> 解压缩路径（Decompression Path）</h3>
<p>当数据从网络接收到 NIC 时，会经过解压缩路径：</p>
<p><strong>组件顺序：</strong> <strong>Rx DEMUX</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>解压缩器</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>解分组暂存缓冲区</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>解分组器</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>Delta 解码器</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>Rx MUX/DMA 引擎+缓冲区</strong></p>
<table>
<thead>
<tr>
<th><strong>组件</strong></th>
<th><strong>功能</strong></th>
<th><strong>机制/实现细节</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Rx DEMUX</strong></td>
<td><strong>流量控制/解压缩判断</strong></td>
<td>从协议引擎接收数据包后，检查其自定义标头中的 <strong>1 位压缩标志</strong>： 1. 如果未压缩，直接通过 Rx MUX 转发给 DMA 引擎+缓冲区（不解压）。 2. 如果已压缩，将数据包导向解压缩器。</td>
</tr>
<tr>
<td><strong>解压缩器</strong></td>
<td><strong>无损解压缩</strong></td>
<td>对数据包的有效载荷进行 <strong>LZ4 解压缩</strong>。解压后的数据被写入<strong>解分组暂存缓冲区</strong>。</td>
</tr>
<tr>
<td><strong>解分组暂存缓冲区</strong></td>
<td><strong>缓存</strong></td>
<td>存储解压缩后的有效载荷。</td>
</tr>
<tr>
<td><strong>解分组器</strong></td>
<td><strong>位级别恢复</strong></td>
<td>将解压缩后的比特或字节序列<strong>重新排列</strong>回其<strong>原始顺序</strong>（即 Delta 值的顺序）。并将解分组后的 Delta 值与<strong>附带的基值</strong>一起发送给 Delta 解码器。</td>
</tr>
<tr>
<td><strong>Delta 解码器</strong></td>
<td><strong>值级别恢复</strong></td>
<td>对解分组器传来的每个 <strong>Delta 值</strong>，执行 <strong>加法运算</strong>： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>原始值</mtext><mo>=</mo><mtext>Delta 值</mtext><mo>+</mo><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\text{原始值} = \text{Delta 值} + D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord cjk_fallback">原始值</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord text"><span class="mord">Delta </span><span class="mord cjk_fallback">值</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，从而完全恢复原始的梯度或激活值。</td>
</tr>
<tr>
<td><strong>Rx MUX</strong></td>
<td><strong>出口</strong></td>
<td>将恢复后的原始数据包发送给 DMA 引擎+缓冲区，最终传输到接收方的 GPU 内存。</td>
</tr>
</tbody>
</table>
<h3 id="asic实现"><a class="markdownIt-Anchor" href="#asic实现"></a> Asic实现</h3>
<p><strong>FPGA 实现（基准）：</strong></p>
<p>最初的实现基于 <strong>Xilinx U280 FPGA</strong>（采用 16nm 工艺）。</p>
<p>目前以 <strong>300 MHz</strong> 的时钟频率运行。</p>
<p><strong>ASIC 预估（目标）：</strong></p>
<p>通过使用 <strong>7nm 工艺</strong>，项目团队预估该设计可以优化并达到高达 <strong>1 GHz</strong> 的时钟频率。</p>
<p><strong>芯片面积：</strong> 预估面积约为 <strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><msup><mtext>mm</mtext><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">2 \text{mm}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord"><span class="mord text"><span class="mord">mm</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></strong>。</p>
<p>这面积不到主流 Mellanox CX-7 NIC 芯片物理面积的 <strong>0.1%</strong>。</p>
<p><strong>功耗：</strong> 功耗约为 <strong>0.5 W</strong>。</p>
<p>这仅占 CX-7 NIC 热设计功耗<strong>25 W</strong> 的大约 <strong>2%</strong>。</p>
<h1 id="efficiency-of-distributed-training"><a class="markdownIt-Anchor" href="#efficiency-of-distributed-training"></a> Eﬀiciency of Distributed Training</h1>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026175746443.png" alt="image-20251026175746443" /></p>
<blockquote>
<p>一篇分析<strong>大规模分布式大语言模型训练</strong>性能、功耗与热行为的论文，研究方法感觉不需要太关注，它的结论还是比较有意思的，感觉是面向特别大规模计算中心的，估计自己这辈子都没机会用上(˘•ω•˘)</p>
</blockquote>
<h2 id="扩展策略scale-up-vs-scale-out"><a class="markdownIt-Anchor" href="#扩展策略scale-up-vs-scale-out"></a> 扩展策略（Scale-up vs. Scale-out）</h2>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026180929133.png" alt="image-20251026180929133" /></p>
<table>
<thead>
<tr>
<th><strong>模型类型</strong></th>
<th><strong>最佳硬件</strong></th>
<th><strong>优势原因</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>小且计算密集型</strong>（如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Llama3-70B</mtext><mo separator="true">,</mo><mtext>Mixtral-8x7B</mtext></mrow><annotation encoding="application/x-tex">\text{Llama3-70B}, \text{Mixtral-8x7B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">Llama3-70B</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">Mixtral-8x7B</span></span></span></span></span>）</td>
<td><strong>Scale-out</strong>（横向扩展，如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn><mo>×</mo><mtext>H100</mtext></mrow><annotation encoding="application/x-tex">64 \times \text{H100}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">6</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H100</span></span></span></span></span>）</td>
<td>具备<strong>更高的聚合计算能力</strong>。对于这些计算受限的模型，更多的 GPU 核心直接转化为更高的性能（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>H100</mtext></mrow><annotation encoding="application/x-tex">\text{H100}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H100</span></span></span></span></span> 在计算上花费的时间更少）。</td>
</tr>
<tr>
<td><strong>大且通信密集型</strong>（如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>GPT3-175B</mtext><mo separator="true">,</mo><mtext>Mixtral-8x22B</mtext></mrow><annotation encoding="application/x-tex">\text{GPT3-175B}, \text{Mixtral-8x22B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">GPT3-175B</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">Mixtral-8x22B</span></span></span></span></span>）</td>
<td><strong>Scale-up</strong>（纵向扩展，如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo>×</mo><mtext>H200</mtext></mrow><annotation encoding="application/x-tex">32 \times \text{H200}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H200</span></span></span></span></span>）</td>
<td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>H200</mtext></mrow><annotation encoding="application/x-tex">\text{H200}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H200</span></span></span></span></span> 拥有 <strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.76</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.76\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">7</span><span class="mord">6</span><span class="mord">×</span></span></span></span> 更大的显存</strong>和<strong>更少的节点</strong>。这减少了跨节点通信，提升了<strong>通信局部性（node-locality）</strong>，使其性能可以匹敌甚至超越 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>H100</mtext></mrow><annotation encoding="application/x-tex">\text{H100}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H100</span></span></span></span></span>。</td>
</tr>
</tbody>
</table>
<p>在通信量大的场景（例如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>GPT3-175B</mtext></mrow><annotation encoding="application/x-tex">\text{GPT3-175B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">GPT3-175B</span></span></span></span></span> 的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>TP2-PP16</mtext></mrow><annotation encoding="application/x-tex">\text{TP2-PP16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">TP2-PP16</span></span></span></span></span> 配置），<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>H200</mtext></mrow><annotation encoding="application/x-tex">\text{H200}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H200</span></span></span></span></span> 能够以<strong>更少的总 GPU 数量</strong>（仅 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>H100</mtext></mrow><annotation encoding="application/x-tex">\text{H100}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">H100</span></span></span></span></span> 的一半）达到相当的吞吐量和更优的<strong>单位 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Token</mtext></mrow><annotation encoding="application/x-tex">\text{Token}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord text"><span class="mord">Token</span></span></span></span></span> 能耗</strong>。</p>
<blockquote>
<p>感觉是没什么意外的结果</p>
</blockquote>
<h2 id="并行策略"><a class="markdownIt-Anchor" href="#并行策略"></a> 并行策略</h2>
<h3 id="tp-pp"><a class="markdownIt-Anchor" href="#tp-pp"></a> TP + PP</h3>
<p><strong>TP所需的集合通信和PP所需的点对点通信，会交织成一种“稀疏、非连续”的通信模式</strong>。</p>
<ul>
<li><strong>缺乏数据分块</strong>：框架发出的通信操作是大量、细粒度的。例如，不是将一个大张量一次性发送，而是分解成许多小张量的Send/Recv操作。</li>
<li><strong>无法充分利用PCIe带宽</strong>：PCIe等高速总线在传输大块连续数据时效率最高。频繁、小批量的通信请求会导致：
<ul>
<li><strong>高延迟</strong>：每个小请求都有开销。</li>
<li><strong>低带宽利用率</strong>：总线在多个小任务间切换，无法“饱和”运行，就像用大卡车一次次运送小包裹，卡车大部分时间是空的。</li>
</ul>
</li>
<li><strong>加剧资源争用</strong>：这些零散的通信任务会争抢有限的PCIe通道和网络接口卡资源，进一步增加延迟。</li>
</ul>
<h3 id="pp-heavy-配置"><a class="markdownIt-Anchor" href="#pp-heavy-配置"></a> PP-heavy 配置</h3>
<p>在PP-Heavy（如<code>TP2-PP16</code>）配置中，流水线阶段很多，每个阶段内的模型切片较薄。</p>
<ul>
<li><strong>通信集中化</strong>：每个GPU只与它相邻的两个GPU（前一个阶段和后一个阶段）进行通信。通信路径非常固定和简单。</li>
<li><strong>大块数据传输</strong>：由于每个流水线阶段要处理整个微批次的激活值，在阶段间传输的<strong>数据块（Payload）更大</strong>。</li>
</ul>
<h3 id="tpep-heavy-配置"><a class="markdownIt-Anchor" href="#tpep-heavy-配置"></a> TP/EP-heavy 配置</h3>
<p><strong>节点内 vs 节点外带宽悬殊</strong>：</p>
<ul>
<li><strong>节点内</strong>：通过NVLink互联，带宽可达<strong>数百GB/s甚至TB/s</strong>。</li>
<li><strong>节点间</strong>：通过InfiniBand或以太网互联，带宽通常只有<strong>100-800 Gb/s（约合10-100 GB/s）</strong>，差了一个数量级。</li>
</ul>
<p><strong>TP/EP-Heavy放大跨节点流量</strong>：</p>
<ul>
<li>如果一个TP=8的组分布在2个节点上（每节点4个GPU），那么每次All-Reduce都有大量数据需要在相对慢速的节点间链路上传输。</li>
<li>EP的All-to-All更是如此，会制造巨大的跨节点通信洪流。<br />
<img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026185826939.png" alt="image-20251026185826939" /></li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left">并行策略</th>
<th style="text-align:left">通信模式</th>
<th style="text-align:left">主要瓶颈</th>
<th style="text-align:left">对硬件拓扑的敏感性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>TP + PP 组合</strong></td>
<td style="text-align:left">稀疏、细粒度的集合通信 + 点对点通信</td>
<td style="text-align:left">PCIe/NIC带宽<strong>利用率低下</strong>，延迟高</td>
<td style="text-align:left">高。通信模式与物理链路不匹配。</td>
</tr>
<tr>
<td style="text-align:left"><strong>PP-Heavy</strong></td>
<td style="text-align:left">集中、粗粒度的点对点通信</td>
<td style="text-align:left">流水线<strong>气泡</strong></td>
<td style="text-align:left">低。通信高效，但要注意阶段负载均衡。</td>
</tr>
<tr>
<td style="text-align:left"><strong>TP/EP-Heavy</strong></td>
<td style="text-align:left">密集、频繁的集合通信 / All-to-All</td>
<td style="text-align:left"><strong>跨节点带宽</strong> 成为天花板</td>
<td style="text-align:left">极高。必须尽可能将通信组约束在单个节点内。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>天哪，10-100 GB都嫌慢了，AI真烧钱</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026185705673.png" alt="image-20251026185705673" /></p>
</blockquote>
<h2 id="微批次大小"><a class="markdownIt-Anchor" href="#微批次大小"></a> 微批次大小</h2>
<p>增加微批次大小并非总是有效的手段 。</p>
<ul>
<li>
<p>超过最优值后，效率会降低，因为<strong>通信带宽饱和</strong>、计算回报递减以及<strong>突发执行模式</strong> 。</p>
</li>
<li>
<p>突发执行模式会提高<strong>峰值功耗</strong>和芯片温度，加剧<strong>热节流</strong>效应</p>
<h4 id="nvidia-集群h200h100"><a class="markdownIt-Anchor" href="#nvidia-集群h200h100"></a> NVIDIA 集群（H200/H100）</h4>
<ul>
<li>在<strong>张量并行</strong>或 <strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>FSDP</mtext></mrow><annotation encoding="application/x-tex">\text{FSDP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">FSDP</span></span></span></span></span> 主导</strong>的配置中（如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>TP8-FSDP</mtext></mrow><annotation encoding="application/x-tex">\text{TP8-FSDP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">TP8-FSDP</span></span></span></span></span>），增大 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MBS</mtext></mrow><annotation encoding="application/x-tex">\text{MBS}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MBS</span></span></span></span></span> 能有效提高计算与通信的比率，性能得到提升。</li>
<li>在<strong>流水线并行</strong>密集型配置中（如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>TP2-PP16</mtext></mrow><annotation encoding="application/x-tex">\text{TP2-PP16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">TP2-PP16</span></span></span></span></span>），增大 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MBS</mtext></mrow><annotation encoding="application/x-tex">\text{MBS}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MBS</span></span></span></span></span> 导致效率急剧下降。</li>
<li><strong>原因：</strong> 流水线停顿的出现，使得执行模式更加<strong>突发</strong>，<strong>间歇性地未充分利用</strong>计算资源。</li>
</ul>
<h4 id="2-amd-mi250-集群"><a class="markdownIt-Anchor" href="#2-amd-mi250-集群"></a> 2. AMD MI250 集群</h4>
<ul>
<li><strong>普遍提升：</strong> 对于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MI250</mtext></mrow><annotation encoding="application/x-tex">\text{MI250}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MI250</span></span></span></span></span>，增大 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MBS</mtext></mrow><annotation encoding="application/x-tex">\text{MBS}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MBS</span></span></span></span></span> 通常会提高训练效率。</li>
<li><strong>根本原因：</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MI250</mtext></mrow><annotation encoding="application/x-tex">\text{MI250}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MI250</span></span></span></span></span> 的瓶颈首先出现在<strong>内存容量</strong>上，而不是像 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>NVIDIA</mtext></mrow><annotation encoding="application/x-tex">\text{NVIDIA}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">NVIDIA</span></span></span></span></span> 集群那样容易达到<strong>热量应力</strong>的限制。</li>
<li><strong>正向循环：</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MBS</mtext></mrow><annotation encoding="application/x-tex">\text{MBS}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MBS</span></span></span></span></span> 增大 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> 工作负载更计算密集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>GPU</mtext></mrow><annotation encoding="application/x-tex">\text{GPU}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">GPU</span></span></span></span></span> 提升时钟频率 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>SM</mtext></mrow><annotation encoding="application/x-tex">\text{SM}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">SM</span></span></span></span></span>利用率和整体效率提高。</li>
</ul>
<h4 id="3-硬件压力与效率的脱钩"><a class="markdownIt-Anchor" href="#3-硬件压力与效率的脱钩"></a> 3. 硬件压力与效率的脱钩</h4>
<ul>
<li><strong>峰值压力持续上升：</strong> 无论训练吞吐量是否提高，<strong>峰值功率</strong>和<strong>热水平</strong>都随着 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>MBS</mtext></mrow><annotation encoding="application/x-tex">\text{MBS}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">MBS</span></span></span></span></span> 的增大而持续上升。</li>
<li>系统效应：
<ul>
<li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>TP</mtext></mrow><annotation encoding="application/x-tex">\text{TP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">TP</span></span></span></span></span> 密集配置：</strong> 峰值功率过高 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> 触发<strong>时钟频率限制</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> 降低计算效率。</li>
<li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>PP</mtext></mrow><annotation encoding="application/x-tex">\text{PP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">PP</span></span></span></span></span> 密集配置：</strong> 流水线停顿 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> <strong>突发执行模式</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> 计算资源被间歇性浪费。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026190714254.png" alt="image-20251026190714254" /></p>
<h2 id="热不平衡"><a class="markdownIt-Anchor" href="#热不平衡"></a> 热不平衡</h2>
<ul>
<li>
<p>服务器前后GPU因气流不均存在显著温差（最高达27%）。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026191805343.png" alt="image-20251026191805343" /></p>
</li>
<li>
<p>热节流导致频率下降，形成“拖后腿”GPU，影响整体同步与效率。</p>
</li>
<li>
<p>通过<strong>热感知调度</strong>（如将冷GPU分配至计算密集型阶段）可提升效率与热均衡</p>
</li>
</ul>
<blockquote>
<p>人话：让不热的GPU多干点活，温度匀一下（通风口的位置，O(∩_∩)O，莫名有点生草）</p>
</blockquote>
<h2 id="优化技术"><a class="markdownIt-Anchor" href="#优化技术"></a> 优化技术</h2>
<h3 id="激活重计算"><a class="markdownIt-Anchor" href="#激活重计算"></a> 激活重计算</h3>
<p>在模型的前向传播过程中，<strong>不保存所有中间结果</strong>，而是在反向传播需要时<strong>临时重新计算</strong>这些结果</p>
<p><strong>解锁内存受限配置</strong>：对于Mixtral-8x22B，它使得 <code>E8-T1-P4</code> 配置成为可能，从而实现了<strong>超过2倍的训练效率提升</strong>，通过牺牲计算来换取内存空间，从而<strong>扩展了可行的并行策略设计空间</strong></p>
<p><strong>普遍的性能下降</strong>，毕竟时间换空间嘛</p>
<h3 id="通信计算重叠"><a class="markdownIt-Anchor" href="#通信计算重叠"></a> 通信计算重叠</h3>
<p>将通信操作与计算操作并行执行，以“隐藏”通信延迟。</p>
<p>能够<strong>提升通信密集型场景的效率</strong>，但<strong>在PP-heavy配置中失效甚至有害</strong>：通信和计算同时进行，会争抢GPU的<strong>内存带宽</strong>和<strong>流多处理器</strong> 等共享资源。这反而可能导致<strong>计算内核的执行时间延长</strong></p>
<p>没从根本上解决有限的网络带宽。</p>
<h3 id="lora微调"><a class="markdownIt-Anchor" href="#lora微调"></a> LoRA微调</h3>
<p>冻结预训练模型的大部分权重，只训练注入到模型中的少量低秩适配器</p>
<p>由于绝大部分模型参数被冻结，无需计算梯度和更新，<strong>计算强度和内存访问强度都大幅降低</strong>,与全参数训练相比，LoRA微调期间的<strong>GPU功耗、温度和时钟频率都显著更低</strong>,可以在内存更小、功耗限制更严格的GPU上运行。</p>
<p>代价是<strong>模型表达能力可能受到限制</strong>。虽然LoRA在很多任务上表现接近全量微调，但对于某些复杂任务，其性能可能无法达到全参数训练的水平。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251026192901357.png" alt="image-20251026192901357" /></p>
<blockquote>
<p>网上查到介绍原理的图，感觉这个使用还挺普遍的</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1963969812788126050">(33 封私信 / 80 条消息) 一文彻底搞懂LoRA！为什么大厂都用它微调？低成本实战指南→建议收藏！ - 知乎</a></p>
<p>感觉这个原理写的挺好的，有空可以看看</p>
</blockquote>
<h1 id="skipreduce"><a class="markdownIt-Anchor" href="#skipreduce"></a> SkipReduce</h1>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029135026682.png" alt="image-20251029135026682" /></p>
<p>在深度学习（DNN）领域，模型和数据集的规模正以前所未有的速度增长 。为了应对这种复杂性，分布式训练（即将训练任务分配给多个GPU或节点）已成为必需 。</p>
<p>然而，分布式训练引入了新的瓶颈：<strong>通信开销</strong> 。在使用数据并行时，所有“工人”（workers）在每次迭代后都需要通过 AllReduce 集合通信操作来同步梯度（gradients） 。由于训练是同步的，每个工人都必须等待通信完成才能进入下一次迭代 ，这直接导致了计算资源的浪费 和训练速度的瓶颈 。</p>
<p>在训练过程中，梯度天然是“稀疏”的 ，其数值分布集中在零附近 。这意味着并非所有梯度数据都对模型收敛至关重要 。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/17201336684">(33 封私信 / 80 条消息) 分布式训练中All-Reduce、All-Gather、Reduce-Scatter原理介绍 - 知乎</a></p>
<p>查了下，大概明白分布式训练的原理了，可以作为前置知识</p>
</blockquote>
<h2 id="现有方案的局限"><a class="markdownIt-Anchor" href="#现有方案的局限"></a> <strong>现有方案的局限</strong></h2>
<p>此前的研究也注意到了这一点，并提出了各种压缩方法来减少通信量：</p>
<h3 id="top-k-稀疏化"><a class="markdownIt-Anchor" href="#top-k-稀疏化"></a> <strong>Top-k 稀疏化</strong></h3>
<p>只发送幅度最大的 k% 的梯度 。但这种方法需要以特殊的稀疏格式（COO）发送，这带来了额外的索引开销（导致消息翻倍） ，并且难以集成到现有的通信库（如NCCL）中 。</p>
<h3 id="powersgd"><a class="markdownIt-Anchor" href="#powersgd"></a> <strong>PowerSGD：</strong></h3>
<p>使用低秩近似来压缩梯度 。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029135934530.png" alt="image-20251029135934530" /></p>
<p>上述这些方法虽然减少了通信数据量，但引入了<strong>显著的计算开销</strong>（用于压缩、索引或重建） 。在早期的低带宽网络中，这些开销是值得的。但在现代HPC系统的高带宽（如16 GBps）环境下，<strong>这些计算开销本身成为了新的瓶颈</strong>，甚至抵消了通信节省带来的好处 。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029141455729.png" alt="image-20251029141455729" /></p>
<blockquote>
<p>SkipReduce不进行复杂的压缩，而是通过简单地“跳过” AllReduce 中的某些通信步骤，来“成批”地丢弃梯度，从而直接减少通信时间 。</p>
</blockquote>
<h2 id="skipreduce-算法实现"><a class="markdownIt-Anchor" href="#skipreduce-算法实现"></a> SkipReduce 算法实现</h2>
<table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>AllReduce_Dropout (细粒度跳过)</strong></th>
<th><strong>SkipReduce (粗粒度/切片级跳过)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>目标</strong></td>
<td>减少<strong>消息携带的信息内容</strong>（梯度信息）</td>
<td>减少<strong>通信时间</strong>（通过跳过步骤）</td>
</tr>
<tr>
<td><strong>跳过单位</strong></td>
<td><strong>单个梯度元素</strong>（基于伯努利过程）</td>
<td><strong>梯度切片</strong>（Ring AllReduce 中的一步/迭代）</td>
</tr>
<tr>
<td><strong>通信量</strong></td>
<td><strong>消息大小保持不变</strong></td>
<td><strong>通信步骤减少</strong>，直接缩短了集体操作的持续时间</td>
</tr>
<tr>
<td><strong>实现</strong></td>
<td>在归约核中对每个梯度元素进行随机决策（类似 Dropout）</td>
<td><strong>减少 Reduce-Scatter 循环的迭代次数</strong></td>
</tr>
</tbody>
</table>
<p>如果决定跳过 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span> 步 Reduce-Scatter 步骤，则 Reduce-Scatter 阶段只执行 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo>−</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N - 1 - S)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span></span></span></span> 步。</p>
<p>每跳过一步 Reduce-Scatter，就相当于<strong>跳过了每个 GPU 的一个完整的梯度切片</strong>的最终归约。</p>
<h3 id="static-skipreduce"><a class="markdownIt-Anchor" href="#static-skipreduce"></a> Static SkipReduce</h3>
<p>每次迭代中，所有 GPU <strong>跳过相同的、固定的梯度切片</strong>。</p>
<p><strong>问题：</strong> 导致<strong>偏差</strong>（或不公平性），因为相同的特定切片永远不会对集体通信结果做出贡献，可能损害模型准确性。</p>
<h3 id="random-skipreduce"><a class="markdownIt-Anchor" href="#random-skipreduce"></a> Random SkipReduce</h3>
<p>在每次训练迭代中，通过引入一个<strong>随机偏移量</strong>来移动 Reduce-Scatter 的切片索引， 确保<strong>每次迭代中跳过的切片是变化的</strong>，从而保证所有梯度切片都有机会参与到归约中，促进随机性。</p>
<p><strong>同步挑战与解决方案：</strong> 所有 GPU 必须生成相同的随机数来保证索引同步。通过修改 NCCL，将<strong>当前迭代计数</strong>作为<strong>通用种子</strong>从主机传递给所有 GPU 来解决，避免了额外的同步开销。</p>
<p>在保持几乎相同的通信加速（仅高了 1.1% 的开销）的同时，<strong>显著提高了模型准确率</strong>，其中随机SkipReduce在75%的跳过率下将测试准确率提高了19个百分点。甚至在高跳过率下能与 AllReduce_Dropout 的精度相匹配。</p>
<h3 id="selective-skipreduce"><a class="markdownIt-Anchor" href="#selective-skipreduce"></a> Selective SkipReduce</h3>
<p>梯度稀疏性在模型的不同层之间是不均匀的，如某些层中大梯度幅度的集中所示。主要结论由于梯度的幅度与其重要性相关，因此具有高浓度大梯度幅度的层可以被认为更重要，因此对跳过更敏感。相反，具有较少重要梯度的层不太敏感，是跳过的更好候选者。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029191104645.png" alt="image-20251029191104645" /></p>
<p>通过跳过大的、不重要的层，同时保护小的、关键的层以维持训练精度，从而实现显著的通信加速。</p>
<p>虽然理解为什么某些层具有相对更高的重要性可能需要严格的理论理解，并且超出了本文的范围，但某些层的重要性可以是直观的。例如，我们可以预期VGG-19中的第一个卷积层和transformers中的嵌入层对跳过更敏感，因为它们启动了输入到网络潜在空间的投影。</p>
<blockquote>
<p>有点绷不住，哈哈</p>
</blockquote>
<h2 id="结果"><a class="markdownIt-Anchor" href="#结果"></a> 结果</h2>
<h3 id="时间-精度加速"><a class="markdownIt-Anchor" href="#时间-精度加速"></a> 时间-精度加速</h3>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029192912960.png" alt="image-20251029192912960" /></p>
<p><strong>对比基线 AllReduce：</strong> 在所有工作负载和运行中，SkipReduce 都能够<strong>胜过基线 AllReduce</strong>，并实现了相对于基线 AllReduce 的中值加速。</p>
<p><strong>对比 PowerSGD：</strong> SkipReduce 在五次运行中实现了相对于 PowerSGD <strong>平均 16% 的 TTA 加速</strong>。</p>
<h3 id="迭代时间对比"><a class="markdownIt-Anchor" href="#迭代时间对比"></a> 迭代时间对比</h3>
<p>在 LLaMA3.2 上，SkipReduce 的迭代时间比 PowerSGD <strong>慢 6%</strong></p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029193046136.png" alt="image-20251029193046136" /></p>
<h3 id="跳过-reduce-scatter-rs-与-allgather-ag-的权衡"><a class="markdownIt-Anchor" href="#跳过-reduce-scatter-rs-与-allgather-ag-的权衡"></a> 跳过 Reduce-Scatter (RS) 与 AllGather (AG) 的权衡</h3>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251029193359456.png" alt="image-20251029193359456" /></p>
<table>
<thead>
<tr>
<th><strong>方案</strong></th>
<th><strong>跳过步骤</strong></th>
<th><strong>性能影响</strong></th>
<th><strong>准确率损失</strong></th>
<th><strong>关键发现</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>方案一 (SkipReduce 现有)</strong></td>
<td>跳过 <strong>50% 的 Reduce-Scatter (RS)</strong></td>
<td><strong>收敛速度快</strong></td>
<td>相对较小</td>
<td>仅针对 RS 阶段可以减少通信时间。</td>
</tr>
<tr>
<td><strong>方案二 (扩展 SkipReduce)</strong></td>
<td>跳过 <strong>25% 的 RS 和 25% 的 AllGather (AG)</strong></td>
<td><strong>显著降低收敛速度</strong></td>
<td>训练准确率降低 <strong>1.95</strong> 个点；测试准确率降低 <strong>1.3</strong> 个点。</td>
<td><strong>跳过 AllGather 的代价更高</strong>，因为它涉及所有其他 GPU 的信息，导致更高的信息损失。</td>
</tr>
</tbody>
</table>
<h1 id="torus-networks"><a class="markdownIt-Anchor" href="#torus-networks"></a> Torus Networks</h1>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251031091603164.png" alt="image-20251031091603164" /></p>
<h2 id="问题"><a class="markdownIt-Anchor" href="#问题"></a> 问题</h2>
<p>All-to-All集合通信，由于其复杂的点对点通信模式和阻塞特性，已成为分布式DLRM和MoE加速中的主要性能瓶颈。此外，长时间的分布式处理经常遇到链路故障，与支持任意到任意连接（如Clos网络）的交换式拓扑不同，环形网络上的All-to-All通信会因共享路由路径而相互干扰，从而造成关键的性能限制。</p>
<p>无故障情况下，我们提出了 HalfRing 算法和 DimRotation 调度。HalfRing 利用双向链路在环上构建最短通信路径，而 DimRotation 在多个维度上分配每个数据块的通信序列，以实现完整的带宽利用率。在有故障的情况下，我们引入了 FoldedRing 算法和 MATE 调度。FoldedRing 促进环上的容错通信，而 MATE 通过利用来自其他维度的可用链路来加速故障环上的通信。</p>
<h2 id="无故障场景优化"><a class="markdownIt-Anchor" href="#无故障场景优化"></a> 无故障场景优化</h2>
<h3 id="halfring-算法单维"><a class="markdownIt-Anchor" href="#halfring-算法单维"></a> HalfRing 算法（单维）</h3>
<h4 id="ring算法的问题"><a class="markdownIt-Anchor" href="#ring算法的问题"></a> Ring算法的问题</h4>
<p>总是沿着固定的方向（比如<strong>顺时针</strong>）进行传输，导致很多通信走的不是最短路径。</p>
<h4 id="改进"><a class="markdownIt-Anchor" href="#改进"></a> 改进</h4>
<p><strong>最短路径优先：</strong></p>
<ul>
<li>对于任何一对通信的节点，HalfRing 不再盲目地只走一个方向。</li>
<li>它会根据发送者和接收者之间的<strong>实际距离</strong>，选择<strong>最短的路径</strong>进行传输。</li>
</ul>
<p><strong>双向同时传输：</strong></p>
<ul>
<li>
<p>由于 HalfRing 确保所有通信都走最短路径，这使得通信链路的利用变得更“干净”：每个通信阶段<strong>只在一个方向上</strong>消耗链路带宽。</p>
</li>
<li>
<p>这样，<strong>另一个方向</strong>的链路（逆时针链路）就可以<strong>完全空闲</strong>下来，用于处理具有<strong>相同跳数</strong>的另一组通信。</p>
</li>
<li>
<p>通过这种方式，HalfRing 实现了<strong>同时利用顺时针和逆时针链路</strong>进行不同的通信任务，且<strong>没有链路冲突</strong></p>
<p>在具有奇数个节点的环中（N = 2k + 1），有 2k 个阶段，并且 All-to-All 可以在 k 对中完成。对于具有偶数个节点的环（N = 2k），有 2k − 1 个阶段，导致一个未配对的阶段，HalfRing 均匀地分割未配对阶段的数据，并将其在两个方向上发送，从而充分利用带宽。</p>
</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251030100940440.png" alt="image-20251030100940440" /></p>
<h3 id="dimrotation-调度多维"><a class="markdownIt-Anchor" href="#dimrotation-调度多维"></a> DimRotation 调度（多维）</h3>
<p>3D环上的All-to-All需要跨X、Y和Z三个维度进行三个阶段，并且每个阶段都充分利用相应维度中所有环的带宽。在每个阶段，每个维度中的环同时传输数据。流水线调度将数据分成多个块，然后以相同的X-Y-Z维度顺序依次通信。流水线通过在不同维度上同时运行多个块来提高带宽利用率。</p>
<p>流水线调度不可避免地会引入气泡，如果块大小较大，流水线无法充分重叠不同维度上不同块的时间，从而导致性能不佳。相反，当块大小较小时，大量的块会引入显著的调度成本和通信初始化开销，从而增加整体延迟。</p>
<p>对于一个N维环面，数据被均匀地分成𝑁块，第𝑖𝑡ℎ块按照维度𝑖𝑡ℎ、𝑖𝑡ℎ+1、…等的顺序进行通信。如图，DimRotation允许三个块执行无冲突、全覆盖的多维通信，从而实现完全的带宽利用率。主要结论同时，块的数量被设置为完全重叠通信所需的最小值，从而显著减少调度开销。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251030104108200.png" alt="image-20251030104108200" /></p>
<blockquote>
<p>简单查了下，维度顺序路由（DOR）规定了严格的维度传输顺序（如 X <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> Y <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> Z）。一旦数据包从 X 维进入 Y 维，它就不能再回到 X 维进行通信</p>
<p>沟槽的五级流水线还在追我，不过这是N级流水线了</p>
</blockquote>
<h2 id="容错算法"><a class="markdownIt-Anchor" href="#容错算法"></a> 容错算法</h2>
<h3 id="单维折叠环"><a class="markdownIt-Anchor" href="#单维折叠环"></a> 单维折叠环</h3>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251030204749486.png" alt="image-20251030204749486" /></p>
<p>当节点 1 和节点 4 之间出现链路故障时，所有逆时针链路带宽都会被重新利用，以补偿故障链路。通信以折叠环方式继续进行。折叠环完成阶段 1 所需的时间是环的两倍。</p>
<h3 id="多维mate调度"><a class="markdownIt-Anchor" href="#多维mate调度"></a> 多维MATE调度</h3>
<p>故障环上的慢速传输导致DimRotation调度中的不匹配。由于故障环，X维度的传输速度减慢，这反过来会影响其他维度的传输，导致整体All-to-All性能下降。</p>
<p>其次，FoldedRing算法只能解决环内的单个链路故障。对于两个或多个故障，FoldedRing无法在故障环上建立连接。因此，尽管故障链路的两个端点之间的通信仍然可以通过其他方向的路由继续进行，但All-to-All仍然被迫中断。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251030213729928.png" alt="image-20251030213729928" /></p>
<p>MATE 利用同一拓扑内<strong>其他 X 维环</strong>和<strong>其他维度（如 Y 维）的链路</strong>，为故障环上的每个节点构建<strong>额外的、无冲突的双向连接</strong>。通过这些额外的链路，MATE 使用高效的 <strong>HalfRing</strong> 算法来传输故障环上的<strong>剩余数据</strong>。同时，故障环本身仍使用 FoldedRing 传输<strong>部分数据</strong>。</p>
<p><img src= "/img/loading.gif" data-lazy-src="../images/MICRO-2025-Systems-for-Al-Training-%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20251030215012209.png" alt="image-20251030215012209" /></p>
<p>通过在这些额外的X维链接和故障环内的FoldedRing上分配数据以进行同时传输，增强了全对全通信。</p>
<table>
<thead>
<tr>
<th><strong>MATE (基础版)</strong></th>
<th><strong>MATEe (增强版)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>完全消除</strong>正常阶段的通信。</td>
<td><strong>分配</strong>一部分数据在正常阶段通过故障环通信。</td>
</tr>
<tr>
<td>承载<strong>全部</strong>需要在故障环上传输的数据。 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> 阶段数据量较大)</td>
<td>承载<strong>剩余</strong>未在正常阶段传输的数据。 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mi>e</mi></msub></mrow><annotation encoding="application/x-tex">M_e</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 阶段数据量较小)</td>
</tr>
<tr>
<td><strong>纯粹分离：</strong> 将故障环的通信完全推迟到加速阶段，利用其他维度资源统一解决。</td>
<td><strong>混合利用：</strong> 充分利用故障环的<strong>残余带宽</strong>，减轻加速阶段的压力。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>废物（FoldedRing）利用的程度的区别</p>
</blockquote>
<h2 id="实现效果"><a class="markdownIt-Anchor" href="#实现效果"></a> 实现效果</h2>
<h3 id="无故障-场景优化效果"><a class="markdownIt-Anchor" href="#无故障-场景优化效果"></a> <strong>无故障 场景优化效果</strong></h3>
<table>
<thead>
<tr>
<th style="text-align:left"><strong>对比项</strong></th>
<th style="text-align:left"><strong>方案</strong></th>
<th style="text-align:left"><strong>性能提升 (平均)</strong></th>
<th style="text-align:left"><strong>备注</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">与基准 (Ring + Pipeline) 对比</td>
<td style="text-align:left">HalfRing 算法</td>
<td style="text-align:left">1.56×</td>
<td style="text-align:left">单独使用</td>
</tr>
<tr>
<td style="text-align:left">与基准 (Ring + Pipeline) 对比</td>
<td style="text-align:left">DimRotation 调度</td>
<td style="text-align:left">1.45×</td>
<td style="text-align:left">单独使用</td>
</tr>
<tr>
<td style="text-align:left">与基准 (Ring + Pipeline) 对比</td>
<td style="text-align:left">HalfRing + DimRotation</td>
<td style="text-align:left">2.28×</td>
<td style="text-align:left">结合使用</td>
</tr>
<tr>
<td style="text-align:left">与 Google TPUv4 DOR 对比</td>
<td style="text-align:left">HalfRing + DimRotation</td>
<td style="text-align:left">1.57×</td>
<td style="text-align:left">在模拟 TPUv4 pod 上</td>
</tr>
<tr>
<td style="text-align:left">真实模型（端到端）性能</td>
<td style="text-align:left">HalfRing + DimRotation vs 基准</td>
<td style="text-align:left">All-to-All: 1.97×，总时间: 1.64×</td>
<td style="text-align:left">DLRM 和 MoE 模型</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="容错-fault-tolerant-场景优化效果"><a class="markdownIt-Anchor" href="#容错-fault-tolerant-场景优化效果"></a> <strong>容错 (Fault-Tolerant) 场景优化效果</strong></h3>
<table>
<thead>
<tr>
<th style="text-align:left"><strong>对比项</strong></th>
<th style="text-align:left"><strong>方案</strong></th>
<th style="text-align:left"><strong>性能提升 (平均)</strong></th>
<th style="text-align:left"><strong>备注</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">与基准 (Ring + Pipeline) 对比</td>
<td style="text-align:left">FoldedRing + Pipeline</td>
<td style="text-align:left">0.55×</td>
<td style="text-align:left">容错但性能下降</td>
</tr>
<tr>
<td style="text-align:left">与基准 (Ring + Pipeline) 对比</td>
<td style="text-align:left">FoldedRing + MATE</td>
<td style="text-align:left">1.36×</td>
<td style="text-align:left">容错且性能提升</td>
</tr>
<tr>
<td style="text-align:left">与基准 (Ring + Pipeline) 对比</td>
<td style="text-align:left">FoldedRing + MATEe</td>
<td style="text-align:left">1.37×</td>
<td style="text-align:left">容错且性能提升</td>
</tr>
<tr>
<td style="text-align:left">与 Google TPUv4 WFR 对比</td>
<td style="text-align:left">MATEe</td>
<td style="text-align:left">1.61×</td>
<td style="text-align:left">All-to-All 带宽饱和时</td>
</tr>
<tr>
<td style="text-align:left">真实模型（端到端）性能</td>
<td style="text-align:left">MATE vs 基准</td>
<td style="text-align:left">All-to-All: 1.24×，总时间: 1.20×</td>
<td style="text-align:left">单链路故障</td>
</tr>
<tr>
<td style="text-align:left">真实模型（端到端）性能</td>
<td style="text-align:left">MATEe vs 基准</td>
<td style="text-align:left">All-to-All: 1.38×，总时间: 1.29×</td>
<td style="text-align:left">单链路故障</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="其他关键效果"><a class="markdownIt-Anchor" href="#其他关键效果"></a> <strong>其他关键效果</strong></h3>
<table>
<thead>
<tr>
<th style="text-align:left"><strong>效果类型</strong></th>
<th style="text-align:left"><strong>方案</strong></th>
<th style="text-align:left"><strong>性能提升或特点</strong></th>
<th style="text-align:left"><strong>场景说明</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">可扩展性</td>
<td style="text-align:left">MATE / MATEe</td>
<td style="text-align:left">性能随维度增加而提升</td>
<td style="text-align:left">从2D到4D网络</td>
</tr>
<tr>
<td style="text-align:left">非均匀通信</td>
<td style="text-align:left">HalfRing + DimRotation vs DOR</td>
<td style="text-align:left">1.27×</td>
<td style="text-align:left">MoE 模型非均匀 All-to-All</td>
</tr>
<tr>
<td style="text-align:left">非均匀通信</td>
<td style="text-align:left">MATEe vs WFR</td>
<td style="text-align:left">1.17×</td>
<td style="text-align:left">故障情况下，MoE 模型</td>
</tr>
<tr>
<td style="text-align:left">多重故障</td>
<td style="text-align:left">MATE vs WFR</td>
<td style="text-align:left">1.14× 到 1.55×</td>
<td style="text-align:left">多种多重链路故障类型</td>
</tr>
</tbody>
</table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://iczjn.com">ZJN</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://iczjn.com/posts/1253395412.html">http://iczjn.com/posts/1253395412.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://iczjn.com" target="_blank">Cosmos</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文</a><a class="post-meta__tags" href="/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">体系结构</a><a class="post-meta__tags" href="/tags/MICRO/">MICRO</a></div><div class="post-share"><div class="social-share" data-image="/img/cover_16.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/3740239699.html" title="Neural_MP论文阅读"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_12.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Neural_MP论文阅读</div></div><div class="info-2"><div class="info-item-1">Neural_MP通过大规模数据驱动的学习，构建一个通用、快速运动规划器，能泛化到未见过的真实世界场景。</div></div></div></a><a class="pagination-related" href="/posts/2274654979.html" title="Window_use环境配置和UI编写"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_15.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Window_use环境配置和UI编写</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/1253343412.html" title="统一高效因子图加速器设计"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_17.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-16</div><div class="info-item-2">统一高效因子图加速器设计</div></div><div class="info-2"><div class="info-item-1">针对机器人优化的统一高效因子图加速器设计</div></div></div></a><a class="pagination-related" href="/posts/1253343453.html" title="3D开放世界通用智能体"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_20.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-18</div><div class="info-item-2">3D开放世界通用智能体</div></div><div class="info-2"><div class="info-item-1">拿着VLA去玩原神(doge)</div></div></div></a><a class="pagination-related" href="/posts/3740239749.html" title="JanusVLN论文阅读"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_19.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-28</div><div class="info-item-2">JanusVLN论文阅读</div></div><div class="info-2"><div class="info-item-1"> VLN几个有点意思的论文 [2025] VLN-R1: 使用大型视觉语言模型 LVLM，结合微调和强化训练，实现连续环境导航 [2025] StreamVLN:  连续导航，通过在线、多轮对话的方式，输入连续视频，输出动作序列 [2025]...</div></div></div></a><a class="pagination-related" href="/posts/3740239699.html" title="Neural_MP论文阅读"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_12.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-20</div><div class="info-item-2">Neural_MP论文阅读</div></div><div class="info-2"><div class="info-item-1">Neural_MP通过大规模数据驱动的学习，构建一个通用、快速运动规划器，能泛化到未见过的真实世界场景。</div></div></div></a><a class="pagination-related" href="/posts/6732346653.html" title="nexus论文阅读"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_28.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-14</div><div class="info-item-2">nexus论文阅读</div></div><div class="info-2"><div class="info-item-1">Decoupled Diffusion Sparks Adaptive Scene Generation 论文阅读</div></div></div></a><a class="pagination-related" href="/posts/3740239499.html" title="π0阅读复现"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover_14.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-18</div><div class="info-item-2">π0阅读复现</div></div><div class="info-2"><div class="info-item-1"> A human being should be able to change a diaper, plan an invasion, butcher a hog, conn a ship, design a building, write a sonnet,...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "/img/loading.gif" data-lazy-src="/img/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">ZJN</div><div class="author-info-description">穷其道者 归处亦同</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/FDU-ZJN"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">嘟嘟噜</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5"><span class="toc-number">1.</span> <span class="toc-text"> 分布式训练策略</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#netzlp"><span class="toc-number"></span> <span class="toc-text"> NetZlP</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95"><span class="toc-number"></span> <span class="toc-text"> 压缩算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BA%A7%E5%88%AB%E8%BD%AC%E6%8D%A2%E5%AD%97%E8%8A%82%E5%92%8C%E6%AF%94%E7%89%B9%E5%88%86%E7%BB%84"><span class="toc-number">1.</span> <span class="toc-text"> 位级别转换：字节和比特分组</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%80%BC%E7%BA%A7%E5%88%AB%E8%BD%AC%E6%8D%A2delta-%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.</span> <span class="toc-text"> 值级别转换：Delta 压缩</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E5%8A%A0%E9%80%9F%E5%99%A8"><span class="toc-number"></span> <span class="toc-text"> 压缩加速器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E8%B7%AF%E5%BE%84"><span class="toc-number">1.</span> <span class="toc-text"> 压缩路径</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%8E%8B%E7%BC%A9%E8%B7%AF%E5%BE%84decompression-path"><span class="toc-number">2.</span> <span class="toc-text"> 解压缩路径（Decompression Path）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#asic%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text"> Asic实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#efficiency-of-distributed-training"><span class="toc-number"></span> <span class="toc-text"> Eﬀiciency of Distributed Training</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A9%E5%B1%95%E7%AD%96%E7%95%A5scale-up-vs-scale-out"><span class="toc-number"></span> <span class="toc-text"> 扩展策略（Scale-up vs. Scale-out）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5"><span class="toc-number"></span> <span class="toc-text"> 并行策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tp-pp"><span class="toc-number">1.</span> <span class="toc-text"> TP + PP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pp-heavy-%E9%85%8D%E7%BD%AE"><span class="toc-number">2.</span> <span class="toc-text"> PP-heavy 配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tpep-heavy-%E9%85%8D%E7%BD%AE"><span class="toc-number">3.</span> <span class="toc-text"> TP&#x2F;EP-heavy 配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E6%89%B9%E6%AC%A1%E5%A4%A7%E5%B0%8F"><span class="toc-number"></span> <span class="toc-text"> 微批次大小</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#nvidia-%E9%9B%86%E7%BE%A4h200h100"><span class="toc-number">0.1.</span> <span class="toc-text"> NVIDIA 集群（H200&#x2F;H100）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-amd-mi250-%E9%9B%86%E7%BE%A4"><span class="toc-number">0.2.</span> <span class="toc-text"> 2. AMD MI250 集群</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%A1%AC%E4%BB%B6%E5%8E%8B%E5%8A%9B%E4%B8%8E%E6%95%88%E7%8E%87%E7%9A%84%E8%84%B1%E9%92%A9"><span class="toc-number">0.3.</span> <span class="toc-text"> 3. 硬件压力与效率的脱钩</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%83%AD%E4%B8%8D%E5%B9%B3%E8%A1%A1"><span class="toc-number"></span> <span class="toc-text"> 热不平衡</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-number"></span> <span class="toc-text"> 优化技术</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E9%87%8D%E8%AE%A1%E7%AE%97"><span class="toc-number">1.</span> <span class="toc-text"> 激活重计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E4%BF%A1%E8%AE%A1%E7%AE%97%E9%87%8D%E5%8F%A0"><span class="toc-number">2.</span> <span class="toc-text"> 通信计算重叠</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lora%E5%BE%AE%E8%B0%83"><span class="toc-number">3.</span> <span class="toc-text"> LoRA微调</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#skipreduce"><span class="toc-number"></span> <span class="toc-text"> SkipReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%B0%E6%9C%89%E6%96%B9%E6%A1%88%E7%9A%84%E5%B1%80%E9%99%90"><span class="toc-number"></span> <span class="toc-text"> 现有方案的局限</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#top-k-%E7%A8%80%E7%96%8F%E5%8C%96"><span class="toc-number">1.</span> <span class="toc-text"> Top-k 稀疏化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#powersgd"><span class="toc-number">2.</span> <span class="toc-text"> PowerSGD：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#skipreduce-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number"></span> <span class="toc-text"> SkipReduce 算法实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#static-skipreduce"><span class="toc-number">1.</span> <span class="toc-text"> Static SkipReduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#random-skipreduce"><span class="toc-number">2.</span> <span class="toc-text"> Random SkipReduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#selective-skipreduce"><span class="toc-number">3.</span> <span class="toc-text"> Selective SkipReduce</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C"><span class="toc-number"></span> <span class="toc-text"> 结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%B6%E9%97%B4-%E7%B2%BE%E5%BA%A6%E5%8A%A0%E9%80%9F"><span class="toc-number">1.</span> <span class="toc-text"> 时间-精度加速</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%AD%E4%BB%A3%E6%97%B6%E9%97%B4%E5%AF%B9%E6%AF%94"><span class="toc-number">2.</span> <span class="toc-text"> 迭代时间对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%B3%E8%BF%87-reduce-scatter-rs-%E4%B8%8E-allgather-ag-%E7%9A%84%E6%9D%83%E8%A1%A1"><span class="toc-number">3.</span> <span class="toc-text"> 跳过 Reduce-Scatter (RS) 与 AllGather (AG) 的权衡</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#torus-networks"><span class="toc-number"></span> <span class="toc-text"> Torus Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98"><span class="toc-number"></span> <span class="toc-text"> 问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E6%95%85%E9%9A%9C%E5%9C%BA%E6%99%AF%E4%BC%98%E5%8C%96"><span class="toc-number"></span> <span class="toc-text"> 无故障场景优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#halfring-%E7%AE%97%E6%B3%95%E5%8D%95%E7%BB%B4"><span class="toc-number">1.</span> <span class="toc-text"> HalfRing 算法（单维）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ring%E7%AE%97%E6%B3%95%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.</span> <span class="toc-text"> Ring算法的问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B"><span class="toc-number">1.2.</span> <span class="toc-text"> 改进</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dimrotation-%E8%B0%83%E5%BA%A6%E5%A4%9A%E7%BB%B4"><span class="toc-number">2.</span> <span class="toc-text"> DimRotation 调度（多维）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%B9%E9%94%99%E7%AE%97%E6%B3%95"><span class="toc-number"></span> <span class="toc-text"> 容错算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E7%BB%B4%E6%8A%98%E5%8F%A0%E7%8E%AF"><span class="toc-number">1.</span> <span class="toc-text"> 单维折叠环</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%BB%B4mate%E8%B0%83%E5%BA%A6"><span class="toc-number">2.</span> <span class="toc-text"> 多维MATE调度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E6%95%88%E6%9E%9C"><span class="toc-number"></span> <span class="toc-text"> 实现效果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E6%95%85%E9%9A%9C-%E5%9C%BA%E6%99%AF%E4%BC%98%E5%8C%96%E6%95%88%E6%9E%9C"><span class="toc-number">1.</span> <span class="toc-text"> 无故障 场景优化效果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%B9%E9%94%99-fault-tolerant-%E5%9C%BA%E6%99%AF%E4%BC%98%E5%8C%96%E6%95%88%E6%9E%9C"><span class="toc-number">2.</span> <span class="toc-text"> 容错 (Fault-Tolerant) 场景优化效果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E5%85%B3%E9%94%AE%E6%95%88%E6%9E%9C"><span class="toc-number">3.</span> <span class="toc-text"> 其他关键效果</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/6732346653.html" title="nexus论文阅读"><img src= "/img/loading.gif" data-lazy-src="/img/cover_28.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="nexus论文阅读"/></a><div class="content"><a class="title" href="/posts/6732346653.html" title="nexus论文阅读">nexus论文阅读</a><time datetime="2025-12-14T00:48:08.000Z" title="发表于 2025-12-14 08:48:08">2025-12-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3513323678.html" title="光线追踪的通俗原理"><img src= "/img/loading.gif" data-lazy-src="/img/cover_29.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="光线追踪的通俗原理"/></a><div class="content"><a class="title" href="/posts/3513323678.html" title="光线追踪的通俗原理">光线追踪的通俗原理</a><time datetime="2025-12-10T02:00:05.000Z" title="发表于 2025-12-10 10:00:05">2025-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/6733346653.html" title="Versatile Behavior Diffusion代码详解"><img src= "/img/loading.gif" data-lazy-src="/img/cover_27.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Versatile Behavior Diffusion代码详解"/></a><div class="content"><a class="title" href="/posts/6733346653.html" title="Versatile Behavior Diffusion代码详解">Versatile Behavior Diffusion代码详解</a><time datetime="2025-11-29T07:25:24.000Z" title="发表于 2025-11-29 15:25:24">2025-11-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/1253334338.html" title="gsutil使用技巧指北"><img src= "/img/loading.gif" data-lazy-src="/img/cover_26.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="gsutil使用技巧指北"/></a><div class="content"><a class="title" href="/posts/1253334338.html" title="gsutil使用技巧指北">gsutil使用技巧指北</a><time datetime="2025-11-29T00:50:08.000Z" title="发表于 2025-11-29 08:50:08">2025-11-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/932177725.html" title="hexo-butterfly主题giscus评论"><img src= "/img/loading.gif" data-lazy-src="/img/cover_25.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="hexo-butterfly主题giscus评论"/></a><div class="content"><a class="title" href="/posts/932177725.html" title="hexo-butterfly主题giscus评论">hexo-butterfly主题giscus评论</a><time datetime="2025-11-28T05:25:08.000Z" title="发表于 2025-11-28 13:25:08">2025-11-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2025 - 2026 By ZJN</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'FDU-ZJN/blog_comment',
      'data-repo-id': 'R_kgDOQemcDg',
      'data-category-id': 'DIC_kwDOQemcDs4CzJof',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax src="https://unpkg.com/oh-my-live2d"></script><script>const oml2d = OML2D.loadOml2d({importType:"cubism5",dockedPosition:"right",mobileDisplay:false,models:[{"path":"/live2d_models/ugofficial/ugofficial.model3.json","position":[0,0],"scale":0.25,"stageStyle":{"width":200,"height":230},"mobilePosition":[-20,-10],"mobileScale":0.06,"mobileStageStyle":{"width":150,"height":140},"motionPreloadStrategy":"ALL"}],tips:{style: {"width":200,"height":70,"left":"calc(50% - 20px)","top":"-100px"},mobileStyle: {"width":150,"left":10,"bottom":10,"top":"auto"},idleTips:{interval:3600,message:["BUG DE完了吗~"]}}});</script><!-- hexo injector body_end end --></body></html>